{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing Pipeline  \n",
    "This Notebook documents the complete data preprocessing workflow.     \n",
    "  \n",
    "The core logic has been implemented in Python scripts within the `src/preprocess_data/` directory.    \n",
    "Because of the large size of the source code, this Notebook serves as the scheduling layer, executing these scripts sequentially while explaining the logic behind each step.  \n",
    "  \n",
    "**⏱️ Estimated runtime (excluding data-download time):** more than 200 Hours  \n",
    "  * **\\>200 Hours**: Get the newest original [ArXiv Dataset](https://www.kaggle.com/Cornell-University/arxiv) and call OpenAlex API to insert citation data.    \n",
    "   * **\\<1 Hours**: For data cleaning."
   ],
   "id": "935270ef76c7840"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**1. Environment & Path Setup**\n",
    " **Prerequisites**:    \n",
    "- Python 3.12+  \n",
    "- When running the Notebook, ensure that the current working directory is `src/jupyter_notebook/` , in order to get the relative paths used below correct.\n",
    "- **Download Data**: Get the newest original [ArXiv Dataset](https://www.kaggle.com/Cornell-University/arxiv) and place `arxiv-metadata-oai-snapshot.json` in `data/preprocess/`."
   ],
   "id": "4eafa39de1919769"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cc4e9896f8b65792"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:16:58.649031Z",
     "start_time": "2025-12-03T20:16:57.871198Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -r ../requirements.txt",
   "id": "f7532d116f18d7a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r ../requirements.txt (line 3)) (2.6.1)\r\n",
      "Requirement already satisfied: jsonschema>=4.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r ../requirements.txt (line 4)) (4.25.1)\r\n",
      "Requirement already satisfied: tqdm>=4.60.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r ../requirements.txt (line 5)) (4.66.4)\r\n",
      "Requirement already satisfied: numpy>=1.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r ../requirements.txt (line 6)) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r ../requirements.txt (line 7)) (1.13.1)\r\n",
      "Requirement already satisfied: scikit-learn>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r ../requirements.txt (line 8)) (1.7.2)\r\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r ../requirements.txt (line 9)) (1.2.1)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.0.0->-r ../requirements.txt (line 3)) (4.2.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.0.0->-r ../requirements.txt (line 3)) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.0.0->-r ../requirements.txt (line 3)) (0.27.0)\r\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.0.0->-r ../requirements.txt (line 3)) (0.11.1)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.0.0->-r ../requirements.txt (line 3)) (2.10.3)\r\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.0.0->-r ../requirements.txt (line 3)) (1.3.0)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.0.0->-r ../requirements.txt (line 3)) (4.12.2)\r\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai>=1.0.0->-r ../requirements.txt (line 3)) (3.7)\r\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.0.0->-r ../requirements.txt (line 3)) (2024.7.4)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.0.0->-r ../requirements.txt (line 3)) (1.0.2)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0->-r ../requirements.txt (line 3)) (0.14.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->-r ../requirements.txt (line 3)) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->-r ../requirements.txt (line 3)) (2.27.1)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.0.0->-r ../requirements.txt (line 4)) (23.1.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.0.0->-r ../requirements.txt (line 4)) (2023.7.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.0.0->-r ../requirements.txt (line 4)) (0.30.2)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.0.0->-r ../requirements.txt (line 4)) (0.10.6)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=1.4.0->-r ../requirements.txt (line 8)) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=1.4.0->-r ../requirements.txt (line 8)) (3.6.0)\r\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**2. Reduce 2.8M papers to a manageable subset relevant to CS** (src/preprocess_data/0_1_reduce_categories.py):",
   "id": "940bab99f33150f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:17:18.529297Z",
     "start_time": "2025-12-03T20:17:02.327755Z"
    }
   },
   "cell_type": "code",
   "source": "%run ../src/preprocess_data/0_1_reduce_categories.py",
   "id": "d4086f6ac4e20831",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "过滤中: 100%|██████████| 2840638/2840638 [00:12<00:00, 226262.27line/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary ===\n",
      "Input:     /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-metadata-oai-snapshot.json\n",
      "Output:    /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data.json\n",
      "Read:      2,840,638\n",
      "Kept:      895,809\n",
      "Skipped:   1,944,829\n",
      "By category:\n",
      "  - Computer Science: 1,238,160\n",
      "  - Statistics: 163,543\n",
      "  - EESS: 116,936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " **3. Add citation counts from OpenAlex using DOI and Title matching (sliced execution)**(`(1_0_add_incite_num)`)  \n",
    " As the complete processing of all slices would exceed 200 hours, we have manually paused the operation here.   \n",
    " The processed files can be get from the releases section of our GitHub repository."
   ],
   "id": "9865a17aa2fb6534"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:20:01.848462Z",
     "start_time": "2025-12-03T20:19:56.991522Z"
    }
   },
   "cell_type": "code",
   "source": "%run ../src/preprocess_data/1_0_add_incite_num.py",
   "id": "55ee250a78f4ffa6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Parameters:\n",
      "    INPUT_FILE  = /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data.json\n",
      "    OUTPUT_FILE = /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data-with-citations_slice6.jsonl\n",
      "    CACHE_FILE  = /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/citation_cache.json\n",
      "    SLICE       = 6/19\n",
      "    SLEEP_SECS  = 0.2\n",
      "[i] Cache loaded: 0 entries\n",
      "[i] Total lines=895809, this slice range=268741 ~ 313530 (total 44790 lines)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Slice 6/19:   0%|          | 7/44790 [00:03<5:55:15,  2.10rec/s] \n",
      " Interrupted, saving cache\n",
      "Slice 6/19:   0%|          | 8/44790 [00:04<6:39:57,  1.87rec/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed: processed=8, written=8\n",
      "Output file: /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data-with-citations_slice6.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**4. Merge distributed processing slices into a unified dataset** (`(1_1_merge_slices)`)  \n",
    "Here, we manually constructed a minimal dataset to test whether Jupyter can run.   \n",
    "The processed files can be get from the releases section of our GitHub repository."
   ],
   "id": "2ae068531eb67b3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:20:35.641441Z",
     "start_time": "2025-12-03T20:20:35.635945Z"
    }
   },
   "cell_type": "code",
   "source": "%run ../src/preprocess_data/1_1_merge_slices",
   "id": "97cc6b812eeb0aee",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging (any order): 100%|██████████| 1/1 [00:00<00:00, 1855.07file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge done → /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data-with-citations-final-dataset.json\n",
      "[i] Files: 1 | Lines: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**5. Fix missing citation data using a multi-stage fallback strategy (Truncated Title/Abstract search)** (`(1_2_retry_record_of_negone)`)  \n",
    "Here, we manually constructed a minimal dataset to test whether Jupyter can run.  \n",
    "The processed files can be get from the releases section of our GitHub repository.\n"
   ],
   "id": "e5fd6910ff55c76a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:22:19.960629Z",
     "start_time": "2025-12-03T20:22:16.735089Z"
    }
   },
   "cell_type": "code",
   "source": "%run ../src/preprocess_data/1_2_retry_record_of_negone.py",
   "id": "5ba916ace6c75364",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Parameters:\n",
      "    INPUT_FILE   = /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data-with-citations_merged_zrk_5.json\n",
      "    OUTPUT_FILE  = /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data-with-citations_final_dataset_odd.json\n",
      "    CACHE_FILE   = /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/citation_cache.json\n",
      "    SLEEP_SECS   = 0.25\n",
      "    TITLE_SIM    = 0.8\n",
      "[i] Cache loaded: 8 entries\n",
      "[i] Records to re-check (citation_count == -1): 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Re-check (-1 only): 100%|██████████| 4/4 [00:03<00:00,  1.25rec/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match summary:\n",
      "  DOI:     attempt=0, ok=0, nf=0\n",
      "  Title:   attempt=4, ok=0, nf=4\n",
      "  Title4:  attempt=4, ok=3, nf=1, skipped=0\n",
      "  Abstract:attempt=1, ok=0, nf=1, skipped=0\n",
      "  Final:   ok=3, neg1=1\n",
      "Second pass done. -1 processed=4, total written=96\n",
      "Output file: /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data-with-citations_final_dataset_odd.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**6. Normalize text data and prepare `processed_content` for embedding/indexing** (`(2_data_filtering)`)  \n",
    "Here, we manually constructed a minimal dataset to test whether Jupyter can run.  \n",
    "The processed files can be get from the releases section of our GitHub repository."
   ],
   "id": "7c6ce62aea4ef76b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:25:50.099187Z",
     "start_time": "2025-12-03T20:25:46.544039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "!{sys.executable} ../src/preprocess_data/2_data_filtering.py"
   ],
   "id": "a317e322b566d86c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch processing for large dataset...\r\n",
      "Processing large dataset in batches of 1000...\r\n",
      "Completed! Total processed: 8 papers\r\n",
      "Saved to: /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data-with-citations-final-dataset_preprocessed.json\r\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b04e07c35985c873"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
