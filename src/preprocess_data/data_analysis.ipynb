{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9a1d9d",
   "metadata": {},
   "source": [
    "# ArXiv Metadata Analysis\n",
    "\n",
    "This notebook analyzes the content and format of the `arxiv-metadata-oai-snapshot.json` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ca5bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 4.54 GB\n",
      "File size: 4649.80 MB\n",
      "File size: 4,875,669,363 bytes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up the file path\n",
    "file_path = \"/work3/s242644/PaperTrail/arxiv-metadata-oai-snapshot.json\"\n",
    "\n",
    "# Check file size and basic info\n",
    "file_size = os.path.getsize(file_path)\n",
    "print(f\"File size: {file_size / (1024**3):.2f} GB\")\n",
    "print(f\"File size: {file_size / (1024**2):.2f} MB\")\n",
    "print(f\"File size: {file_size:,} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1373131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers: 2,840,638\n",
      "Average size per paper: 1716 bytes\n"
     ]
    }
   ],
   "source": [
    "# Count total number of papers\n",
    "with open(file_path, 'r') as f:\n",
    "    line_count = sum(1 for line in f)\n",
    "\n",
    "print(f\"Total number of papers: {line_count:,}\")\n",
    "print(f\"Average size per paper: {file_size / line_count:.0f} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea1a317e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual paper structure analysis:\n",
      "==================================================\n",
      "\n",
      "First paper keys: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed']\n",
      "Number of fields: 14\n",
      "\n",
      "Detailed structure of first paper:\n",
      "----------------------------------------\n",
      "id (str): 0704.0001\n",
      "submitter (str): Pavel Nadolsky\n",
      "authors (str): C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\n",
      "title (str): Calculation of prompt diphoton production cross sections at Tevatron and\n",
      "  LHC energies\n",
      "comments (str): 37 pages, 15 figures; published version\n",
      "journal-ref (str): Phys.Rev.D76:013009,2007\n",
      "doi (str): 10.1103/PhysRevD.76.013009\n",
      "report-no (str): ANL-HEP-PR-07-12\n",
      "categories (str): hep-ph\n",
      "license (NoneType): None\n",
      "abstract (str):   A fully differential calculation in perturbative quantum chromodynamics is\n",
      "presented for the produ...\n",
      "versions (list): [{'version': 'v1', 'created': 'Mon, 2 Apr 2007 19:18:42 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Jul 2007 20:10:27 GMT'}]\n",
      "update_date (str): 2008-11-26\n",
      "authors_parsed (list): [['BalÃ¡zs', 'C.', ''], ['Berger', 'E. L.', ''], ['Nadolsky', 'P. M.', ''], ['Yuan', 'C. -P.', '']]\n",
      "\n",
      "Sample of all papers structure:\n",
      "----------------------------------------\n",
      "\n",
      "Paper 1 keys: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed']\n",
      "Paper 1 ID: 0704.0001\n",
      "Paper 1 title: Calculation of prompt diphoton production cross se...\n",
      "\n",
      "Paper 2 keys: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed']\n",
      "Paper 2 ID: 0704.0002\n",
      "Paper 2 title: Sparsity-certifying Graph Decompositions...\n",
      "\n",
      "Paper 3 keys: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed']\n",
      "Paper 3 ID: 0704.0003\n",
      "Paper 3 title: The evolution of the Earth-Moon system based on th...\n",
      "\n",
      "Paper 4 keys: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed']\n",
      "Paper 4 ID: 0704.0004\n",
      "Paper 4 title: A determinant of Stirling cycle numbers counts unl...\n",
      "\n",
      "Paper 5 keys: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed']\n",
      "Paper 5 ID: 0704.0005\n",
      "Paper 5 title: From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\alpha...\n"
     ]
    }
   ],
   "source": [
    "# Load and examine a sample of papers\n",
    "sample_papers = []\n",
    "with open(file_path, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 10:  # Load first 10 papers\n",
    "            break\n",
    "        sample_papers.append(json.loads(line.strip()))\n",
    "\n",
    "print(\"Sample paper structure:\")\n",
    "print(\"=\" * 50)\n",
    "for i, paper in enumerate(sample_papers[:3]):\n",
    "    print(f\"\\nPaper {i+1}:\")\n",
    "    print(f\"ID: {paper['id']}\")\n",
    "    print(f\"Title: {paper['title'][:100]}...\")\n",
    "    print(f\"Authors: {paper['authors'][:100]}...\")\n",
    "    print(f\"Categories: {paper['categories']}\")\n",
    "    print(f\"Abstract length: {len(paper['abstract'])} characters\")\n",
    "    print(f\"Versions: {len(paper['versions'])}\")\n",
    "    print(f\"Update date: {paper['update_date']}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89129a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistency check across papers:\n",
      "==================================================\n",
      "All unique keys found: ['abstract', 'authors', 'authors_parsed', 'categories', 'comments', 'doi', 'id', 'journal-ref', 'license', 'report-no', 'submitter', 'title', 'update_date', 'versions']\n",
      "Total unique keys: 14\n",
      "\n",
      "Key presence across papers:\n",
      "report-no: present in 5/5 papers\n",
      "categories: present in 5/5 papers\n",
      "comments: present in 5/5 papers\n",
      "authors: present in 5/5 papers\n",
      "title: present in 5/5 papers\n",
      "authors_parsed: present in 5/5 papers\n",
      "license: present in 5/5 papers\n",
      "versions: present in 5/5 papers\n",
      "journal-ref: present in 5/5 papers\n",
      "abstract: present in 5/5 papers\n",
      "update_date: present in 5/5 papers\n",
      "doi: present in 5/5 papers\n",
      "submitter: present in 5/5 papers\n",
      "id: present in 5/5 papers\n"
     ]
    }
   ],
   "source": [
    "# Analyze the complete structure of one paper\n",
    "print(\"Complete structure of a single paper:\")\n",
    "print(\"=\" * 50)\n",
    "sample_paper = sample_papers[0]\n",
    "for key, value in sample_paper.items():\n",
    "    if isinstance(value, str) and len(value) > 100:\n",
    "        print(f\"{key}: {value[:100]}... (length: {len(value)})\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"{key}: {value} (length: {len(value)})\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "860d8e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing actual data structure...\n",
      "==================================================\n",
      "All possible keys in dataset: ['abstract', 'authors', 'authors_parsed', 'categories', 'comments', 'doi', 'id', 'journal-ref', 'license', 'report-no', 'submitter', 'title', 'update_date', 'versions']\n",
      "\n",
      "Data types for each key:\n",
      "report-no: {'str', 'NoneType'}\n",
      "categories: {'str'}\n",
      "comments: {'str', 'NoneType'}\n",
      "authors: {'str'}\n",
      "title: {'str'}\n",
      "authors_parsed: {'list'}\n",
      "license: {'str', 'NoneType'}\n",
      "versions: {'list'}\n",
      "journal-ref: {'str', 'NoneType'}\n",
      "abstract: {'str'}\n",
      "update_date: {'str'}\n",
      "doi: {'str', 'NoneType'}\n",
      "submitter: {'str'}\n",
      "id: {'str'}\n",
      "\n",
      "Null/None value analysis:\n",
      "report-no: 92/100 papers have null/empty values\n",
      "categories: 0/100 papers have null/empty values\n",
      "comments: 13/100 papers have null/empty values\n",
      "authors: 0/100 papers have null/empty values\n",
      "title: 0/100 papers have null/empty values\n",
      "authors_parsed: 0/100 papers have null/empty values\n",
      "license: 87/100 papers have null/empty values\n",
      "versions: 0/100 papers have null/empty values\n",
      "journal-ref: 48/100 papers have null/empty values\n",
      "abstract: 0/100 papers have null/empty values\n",
      "update_date: 0/100 papers have null/empty values\n",
      "doi: 50/100 papers have null/empty values\n",
      "submitter: 0/100 papers have null/empty values\n",
      "id: 0/100 papers have null/empty values\n"
     ]
    }
   ],
   "source": [
    "# Analyze categories distribution\n",
    "categories = []\n",
    "abstract_lengths = []\n",
    "years = []\n",
    "\n",
    "print(\"Analyzing categories and other metadata...\")\n",
    "with open(file_path, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 1000:  # Analyze first 1000 papers for efficiency\n",
    "            break\n",
    "        paper = json.loads(line.strip())\n",
    "        \n",
    "        # Extract categories\n",
    "        if paper['categories']:\n",
    "            categories.extend(paper['categories'].split())\n",
    "        \n",
    "        # Extract abstract length\n",
    "        abstract_lengths.append(len(paper['abstract']))\n",
    "        \n",
    "        # Extract year from update_date\n",
    "        if paper['update_date']:\n",
    "            try:\n",
    "                year = int(paper['update_date'].split('-')[0])\n",
    "                years.append(year)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "print(f\"Analyzed {min(1000, line_count)} papers\")\n",
    "print(f\"Total category mentions: {len(categories)}\")\n",
    "print(f\"Unique categories: {len(set(categories))}\")\n",
    "print(f\"Average abstract length: {sum(abstract_lengths)/len(abstract_lengths):.0f} characters\")\n",
    "print(f\"Year range: {min(years)} - {max(years)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2971700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top categories\n",
    "category_counts = Counter(categories)\n",
    "print(\"\\nTop 20 categories:\")\n",
    "print(\"=\" * 30)\n",
    "for category, count in category_counts.most_common(20):\n",
    "    print(f\"{category}: {count}\")\n",
    "\n",
    "# Year distribution\n",
    "year_counts = Counter(years)\n",
    "print(f\"\\nYear distribution (first 10 years):\")\n",
    "print(\"=\" * 30)\n",
    "for year in sorted(year_counts.keys())[:10]:\n",
    "    print(f\"{year}: {year_counts[year]} papers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7244eb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data format summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ARXIV METADATA FILE FORMAT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"File: arxiv-metadata-oai-snapshot.json\")\n",
    "print(f\"Size: {file_size / (1024**3):.2f} GB ({file_size:,} bytes)\")\n",
    "print(f\"Total papers: {line_count:,}\")\n",
    "print(f\"Format: JSON Lines (one JSON object per line)\")\n",
    "print(f\"Encoding: UTF-8\")\n",
    "print(f\"Average paper size: {file_size / line_count:.0f} bytes\")\n",
    "\n",
    "print(f\"\\nData Structure (per paper):\")\n",
    "print(f\"- id: ArXiv paper ID (string)\")\n",
    "print(f\"- submitter: Submitter name (string)\")\n",
    "print(f\"- authors: Author names (string)\")\n",
    "print(f\"- title: Paper title (string)\")\n",
    "print(f\"- comments: Comments (string or null)\")\n",
    "print(f\"- journal-ref: Journal reference (string or null)\")\n",
    "print(f\"- doi: DOI (string or null)\")\n",
    "print(f\"- report-no: Report number (string or null)\")\n",
    "print(f\"- categories: Subject categories (string)\")\n",
    "print(f\"- license: License URL (string or null)\")\n",
    "print(f\"- abstract: Paper abstract (string)\")\n",
    "print(f\"- versions: List of version objects with 'version' and 'created' fields\")\n",
    "print(f\"- update_date: Last update date (YYYY-MM-DD format)\")\n",
    "print(f\"- authors_parsed: List of parsed author names (list of lists)\")\n",
    "\n",
    "print(f\"\\nKey Statistics:\")\n",
    "print(f\"- Average abstract length: {sum(abstract_lengths)/len(abstract_lengths):.0f} characters\")\n",
    "print(f\"- Year range: {min(years)} - {max(years)}\")\n",
    "print(f\"- Unique categories: {len(set(categories))}\")\n",
    "print(f\"- Most common category: {category_counts.most_common(1)[0][0]} ({category_counts.most_common(1)[0][1]} papers)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966d0f11",
   "metadata": {},
   "source": [
    "## Actual File Structure Analysis\n",
    "\n",
    "This analysis examines the **actual structure** of the `arxiv-metadata-oai-snapshot.json` file by reading and analyzing the real data, not making assumptions about the format.\n",
    "\n",
    "### File Properties:\n",
    "- **Size**: 4.6 GB\n",
    "- **Total Papers**: ~2.84 million papers\n",
    "- **Format**: JSON Lines (one JSON object per line)\n",
    "- **Encoding**: UTF-8\n",
    "- **Average paper size**: ~1,600 bytes\n",
    "\n",
    "### Data Structure Discovery:\n",
    "The notebook above will reveal the actual fields and data types present in the file by:\n",
    "1. Loading sample papers from the file\n",
    "2. Extracting all unique field names\n",
    "3. Analyzing data types for each field\n",
    "4. Checking for null/empty values\n",
    "5. Showing actual sample data\n",
    "\n",
    "### Key Points:\n",
    "- **No assumptions made** about field names or structure\n",
    "- **Empirical analysis** of the actual data\n",
    "- **Dynamic discovery** of all possible fields\n",
    "- **Type analysis** for each field\n",
    "- **Consistency checking** across papers\n",
    "\n",
    "Run the cells above to see the actual structure discovered from your specific file.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
