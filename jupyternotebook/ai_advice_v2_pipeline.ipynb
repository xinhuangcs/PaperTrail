{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# AI Advice V2 End-to-End Notebook\n\nThis notebook mirrors the Python utilities inside `src/ai_advice/v2` so the full planning pipeline can be run and debugged interactively.\nIt intentionally keeps the same helper functions and defaults found in the V2 scripts while making them easy to call from a notebook cell.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from __future__ import annotations\n\nfrom pathlib import Path\nfrom datetime import datetime, UTC, timezone\nfrom typing import Any, Dict, List, Optional\nimport argparse\nimport json\nimport math\nimport os\nimport re\nimport sys\nimport time\nimport uuid\n\nfrom jsonschema import Draft202012Validator\nfrom jsonschema.exceptions import ValidationError\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\n# Make project imports work from the notebook location\nROOT_DIR = Path.cwd()\nsys.path.insert(0, str(ROOT_DIR))\nDATA_DIR = ROOT_DIR / \"data\" / \"ai_advice\"\nRECOMMEND_DIR = ROOT_DIR / \"data\" / \"recommend\"\n\nload_dotenv()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Standardize recommended papers (from `standardize_input.py`)\nThe helpers below reproduce the CLI script so we can select a recommend file, normalize text fields, and write a `standardize_input_*.json` file for later steps.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# --- standardize_input helpers ---\n\ndef newest_recommend_file(input_dir: Path, pattern: str) -> Path:\n    files = sorted(input_dir.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n    if not files:\n        raise FileNotFoundError(f\"No files match: {input_dir}/{pattern}\")\n    return files[0]\n\n\ndef normalize_text(s: Optional[str], max_len: int) -> str:\n    if not s:\n        return \"\"\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s[:max_len]\n\n\ndef pick_query(r: Dict[str, Any], max_len: int) -> str:\n    return normalize_text(r.get(\"query\"), max_len)\n\n\ndef extract_year(r: Dict[str, Any]) -> int:\n    ud = (r.get(\"update_date\") or \"\").strip()\n    if len(ud) >= 4 and ud[:4].isdigit():\n        return int(ud[:4])\n    vers = r.get(\"versions\") or []\n    if vers and isinstance(vers, list):\n        created = vers[0].get(\"created\", \"\")\n        m = re.search(r\"\b(\\d{4})\b\", created or \"\")\n        if m:\n            return int(m.group(1))\n    return 0\n\n\ndef to_minimal_record(r: Dict[str, Any], max_abs_len: int, max_query_len: int) -> Dict[str, Any]:\n    pid = r.get(\"id\") or r.get(\"_id\") or \"\"\n    title = (r.get(\"title\") or \"\").strip()\n    abstract = normalize_text(r.get(\"abstract\") or \"\", max_abs_len)\n    year = extract_year(r)\n    cit = int(r.get(\"citation_count\") or 0)\n\n    if r.get(\"_score\") is not None:\n        score = float(r[\"_score\"])\n    elif r.get(\"score\") is not None:\n        score = float(r[\"score\"])\n    elif r.get(\"sim_score\") is not None:\n        score = float(r[\"sim_score\"])\n    else:\n        score = 0.0\n\n    authors = r.get(\"authors\") or r.get(\"authors_parsed\")\n    categories = r.get(\"categories\")\n    query = pick_query(r, max_query_len)\n\n    return {\n        \"id\": pid,\n        \"title\": title,\n        \"abstract\": abstract,\n        \"year\": year,\n        \"citation_count\": cit,\n        \"score\": score,\n        \"authors\": authors,\n        \"categories\": categories,\n        \"query\": query,\n    }\n\n\ndef build_selected_papers(\n    mode: str = \"application\",\n    max_abstract_chars: int = 3000,\n    max_query_chars: int = 200,\n    output_dir: Path = DATA_DIR,\n    input_dir: Path = RECOMMEND_DIR,\n) -> Path:\n    pattern = f\"recommend_{mode}.json\"\n    latest = newest_recommend_file(input_dir, pattern)\n    seen = set()\n    rows: List[Dict[str, Any]] = []\n\n    with latest.open(\"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            r = json.loads(line)\n            pid = r.get(\"id\") or r.get(\"_id\")\n            if not pid or pid in seen:\n                continue\n            seen.add(pid)\n            rows.append(\n                to_minimal_record(\n                    r,\n                    max_abs_len=max_abstract_chars,\n                    max_query_len=max_query_chars,\n                )\n            )\n\n    timestamp = datetime.now(UTC).strftime(\"%Y-%m-%d_%H%M\")\n    out_path = output_dir / f\"standardize_input_{timestamp}.json\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n    with out_path.open(\"w\", encoding=\"utf-8\") as wf:\n        for rec in rows:\n            wf.write(json.dumps(rec, ensure_ascii=False) + \"\n\")\n\n    print(f\"Processed {len(rows)} papers from {latest.name} -> {out_path}\")\n    return out_path\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Plan schema contract (from `schema_contract.py`)\nGenerate or validate the JSON schema that constrains the model output.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# --- schema_contract helpers ---\n\nPLAN_SCHEMA: Dict[str, Any] = {\n    \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n    \"description\": \"Structured learning plan generated from selected papers for a given goal.\",\n    \"title\": \"PaperTrailLearningPlan\",\n    \"type\": \"object\",\n    \"additionalProperties\": False,\n    \"required\": [\n        \"plan_overview\",\n        \"reading_order\",\n        \"actions\",\n        \"metrics\",\n        \"timeline_weeks\",\n        \"risks\",\n        \"goal\",\n        \"study_level\",\n        \"source_papers\",\n        \"metadata\",\n    ],\n    \"properties\": {\n        \"goal\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 2000},\n        \"study_level\": {\"type\": \"string\", \"enum\": [\"beginner\", \"intermediate\", \"advanced\"]},\n        \"source_papers\": {\n            \"type\": \"array\",\n            \"minItems\": 1,\n            \"maxItems\": 200,\n            \"items\": {\"type\": \"string\", \"minLength\": 1},\n        },\n        \"metadata\": {\n            \"type\": \"object\",\n            \"additionalProperties\": False,\n            \"required\": [\"prompt_version\", \"model\", \"created_at\"],\n            \"properties\": {\n                \"prompt_version\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 100},\n                \"model\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 100},\n                \"created_at\": {\"type\": \"string\", \"format\": \"date-time\"},\n            },\n        },\n        \"plan_overview\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 5000},\n        \"reading_order\": {\n            \"type\": \"array\",\n            \"minItems\": 1,\n            \"maxItems\": 50,\n            \"items\": {\n                \"type\": \"object\",\n                \"additionalProperties\": False,\n                \"required\": [\"paper_id\", \"why_first\", \"key_questions\"],\n                \"properties\": {\n                    \"paper_id\": {\"type\": \"string\", \"minLength\": 1},\n                    \"why_first\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 2000},\n                    \"key_questions\": {\n                        \"type\": \"array\",\n                        \"minItems\": 1,\n                        \"maxItems\": 6,\n                        \"items\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 500},\n                    },\n                },\n            },\n        },\n        \"actions\": {\n            \"type\": \"array\",\n            \"minItems\": 1,\n            \"maxItems\": 50,\n            \"items\": {\n                \"type\": \"object\",\n                \"additionalProperties\": False,\n                \"required\": [\"label\", \"how_to\", \"expected_outcome\"],\n                \"properties\": {\n                    \"label\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 200},\n                    \"how_to\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 3000},\n                    \"expected_outcome\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 2000},\n                },\n            },\n        },\n        \"metrics\": {\n            \"type\": \"array\",\n            \"minItems\": 1,\n            \"maxItems\": 30,\n            \"items\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 200},\n        },\n        \"timeline_weeks\": {\n            \"type\": \"array\",\n            \"minItems\": 1,\n            \"maxItems\": 52,\n            \"items\": {\n                \"type\": \"object\",\n                \"additionalProperties\": False,\n                \"required\": [\"week\", \"focus\", \"deliverable\"],\n                \"properties\": {\n                    \"week\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 104},\n                    \"focus\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 500},\n                    \"deliverable\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 1000},\n                },\n            },\n        },\n        \"risks\": {\n            \"type\": \"array\",\n            \"minItems\": 0,\n            \"maxItems\": 30,\n            \"items\": {\n                \"type\": \"object\",\n                \"additionalProperties\": False,\n                \"required\": [\"risk\", \"mitigation\"],\n                \"properties\": {\n                    \"risk\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 500},\n                    \"mitigation\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 1000},\n                },\n            },\n        },\n    },\n}\n\n\ndef write_schema(schema_path: Path | None = None) -> Path:\n    schema_path = schema_path or DATA_DIR / f\"plan_schema_{datetime.now(UTC).strftime('%Y-%m-%d_%H%M')}.json\"\n    Draft202012Validator.check_schema(PLAN_SCHEMA)\n    schema_path.parent.mkdir(parents=True, exist_ok=True)\n    with schema_path.open(\"w\", encoding=\"utf-8\") as f:\n        json.dump(PLAN_SCHEMA, f, ensure_ascii=False, indent=2)\n    return schema_path\n\n\ndef validate_plan(plan: dict) -> None:\n    try:\n        Draft202012Validator(PLAN_SCHEMA).validate(plan)\n    except ValidationError as e:\n        raise ValueError(f\"Schema validation failed: {e.message}\") from e\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Prompt construction utilities (from `prompts.py`)\nThese helpers build the system/user prompts and the response format used by the generator scripts.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# --- prompts helpers ---\n\nPROMPT_VERSION = \"PT-20251102-1\"\n\nSYSTEM_PROMPT = \"\"\"\nYou are a senior computer scientist and mentor.\nTransform a small set of computer science papers into a structured, actionable learning plan.\n\nHard requirements (read carefully):\n- Output MUST be a single JSON object ONLY (no commentary, no markdown, no code fences).\n- Follow the JSON schema fields EXACTLY:\n  goal, study_level, source_papers, metadata, plan_overview, reading_order, actions, metrics, timeline_weeks, risks.\n- Use ONLY the provided papers. Do NOT invent or cite any paper that is not in the provided list.\n- All values in reading_order[].paper_id MUST be chosen from source_papers,\n  and source_papers MUST include ALL provided paper IDs (no missing IDs, no invented IDs).\n- Be concise and execution-oriented (clear steps, measurable outcomes).\n- If information is insufficient, use fewer items and add a risk item explaining the limitation.\n- Do NOT add any extra fields not defined by the schema.\n\nLanguage and style:\n- When referring to the learner, always address them directly as \"You\".\n- Do NOT describe the learner in the third person (e.g., \"a junior computer science student\").\n\nSchema guidance:\n- goal: copy the user goal faithfully in meaning.\n- study_level: choose from [\"beginner\",\"intermediate\",\"advanced\"]; if unsure, prefer \"intermediate\".\n- source_papers: MUST contain all paper_id strings from the provided list (do not drop any, do not invent IDs).\n- metadata:\n  - prompt_version: provided by tooling.\n  - model: provided by tooling.\n  - created_at: current UTC in ISO8601 (e.g., 2025-11-01T12:34:56Z).\n  - tokens_estimated: include only if values are provided; otherwise omit.\n- plan_overview: 1-2 short paragraphs explaining rationale and overall strategy.\n- reading_order: {paper_id, why_first, key_questions[]} with concrete technical questions.\n- actions: 3-10 items of {label, how_to, expected_outcome}, focusing on reproducible tasks.\n- metrics: 2-8 measurable indicators.\n- timeline_weeks: 2-12 items; week starts at 1.\n- risks: 0-8 items; realistic, technical risks only.\n\nOutput rule:\n- Return ONLY the JSON object that conforms to the provided JSON schema.\n\"\"\".strip()\n\n\ndef clip(text: str, max_chars: int) -> str:\n    if not isinstance(text, str):\n        return \"\"\n    if len(text) <= max_chars:\n        return text\n    return text[: max_chars - 3] + \"...\"\n\n\ndef norm_str(x: Any) -> str:\n    if x is None:\n        return \"\"\n    return str(x)\n\n\ndef load_plan_schema(schema_path: str | Path) -> Dict[str, Any]:\n    with Path(schema_path).open(\"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n\n\ndef build_response_format(schema: Dict[str, Any]) -> Dict[str, Any]:\n    return {\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"name\": schema.get(\"title\", \"PaperTrailLearningPlan\"),\n            \"schema\": schema,\n            \"strict\": True,\n        },\n    }\n\n\ndef build_user_prompt(\n    goal: str,\n    papers: List[Dict[str, Any]],\n    *,\n    audience: str = \"you\",\n    study_level_hint: Optional[str] = None,\n    model_id: str = \"gpt-5\",\n    prompt_version: str = PROMPT_VERSION,\n    max_abstract_chars: int = 1200,\n) -> str:\n    if study_level_hint not in {\"beginner\", \"intermediate\", \"advanced\", None}:\n        study_level_hint = None\n    if not goal:\n        for p in papers:\n            q = (p.get(\"query\") or \"\").strip()\n            if q:\n                goal = q\n                break\n\n    lines: List[str] = []\n    lines.append(f\"Goal: {goal}\")\n    lines.append(f\"Audience: {audience}\")\n    if study_level_hint:\n        lines.append(f\"StudyLevelHint: {study_level_hint}\")\n    lines.append(\"Selected research papers (id, title, authors, year, citation_count, categories, abstract):\")\n    allowed_ids = [norm_str(p.get(\"id\")) for p in papers if p.get(\"id\")]\n    lines.append(\"AllowedPaperIDsJSON: \" + json.dumps(allowed_ids, ensure_ascii=False))\n\n    for idx, p in enumerate(papers, start=1):\n        pid = norm_str(p.get(\"id\"))\n        title = norm_str(p.get(\"title\"))\n        authors = norm_str(p.get(\"authors\"))\n        year = norm_str(p.get(\"year\"))\n        cites = norm_str(p.get(\"citation_count\"))\n        cats = norm_str(p.get(\"categories\"))\n        abstract = clip(norm_str(p.get(\"abstract\")), max_abstract_chars)\n\n        lines.append(f\"{idx}) ID: {pid}\")\n        lines.append(f\"   Title: {title}\")\n        lines.append(f\"   Authors: {authors}\")\n        lines.append(f\"   Year: {year}\")\n        lines.append(f\"   Citations: {cites}\")\n        lines.append(f\"   Categories: {cats}\")\n        lines.append(f\"   Abstract: {abstract}\")\n\n    lines.append(\n        \"Return a single JSON object that strictly conforms to the provided JSON schema. \"\n        \"Set source_papers to contain ALL IDs listed in AllowedPaperIDsJSON (do not drop any, do not invent IDs). \"\n        \"Design reading_order so that EVERY paper_id from source_papers appears at least once, \"\n        \"and the sequence forms a coherent learning path for the user.\"\n    )\n\n    return \"\n\".join(lines)\n\n\ndef build_messages(\n    goal: str,\n    papers: List[Dict[str, Any]],\n    *,\n    audience: str = \"you\",\n    study_level_hint: Optional[str] = None,\n    model_id: str = \"gpt-4.1\",\n    prompt_version: str = PROMPT_VERSION,\n    max_abstract_chars: int = 1200,\n) -> List[Dict[str, str]]:\n    user_prompt = build_user_prompt(\n        goal,\n        papers,\n        audience=audience,\n        study_level_hint=study_level_hint,\n        model_id=model_id,\n        prompt_version=prompt_version,\n        max_abstract_chars=max_abstract_chars,\n    )\n    return [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": user_prompt},\n    ]\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Plan generation (from `generate_plan_v3.py`)\nCall the OpenAI Responses API with the prompts above, validate the structured JSON, and save artifacts/logs.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# --- plan generation helpers ---\n\ndef read_papers_jsonl(path: Path) -> List[Dict[str, Any]]:\n    papers = []\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        for i, line in enumerate(f, 1):\n            s = line.strip()\n            if not s:\n                continue\n            try:\n                papers.append(json.loads(s))\n            except json.JSONDecodeError as e:\n                raise ValueError(f\"bad json at line {i}: {e}\") from e\n    return papers\n\n\ndef pick_goal_from_query(papers: List[Dict[str, Any]]) -> str:\n    for p in papers:\n        q = (p.get(\"query\") or \"\").strip()\n        if q:\n            return q\n    raise ValueError(\"no 'query' found in input papers\")\n\n\ndef assert_reading_order_from_source(plan: dict) -> None:\n    src = set(plan.get(\"source_papers\") or [])\n    bad = [it.get(\"paper_id\") for it in plan.get(\"reading_order\", []) if it.get(\"paper_id\") not in src]\n    if bad:\n        raise ValueError(f\"reading_order contains IDs not in source_papers: {bad}\")\n\n\ndef collect_topics_from_recommend(prefer_view: str = \"default\") -> Optional[str]:\n    if prefer_view:\n        preferred_file = RECOMMEND_DIR / f\"recommend_{prefer_view}.json\"\n        if preferred_file.exists():\n            recommend_file = preferred_file\n        else:\n            recommend_files = sorted(\n                RECOMMEND_DIR.glob(\"recommend_*.json\"),\n                key=lambda p: p.stat().st_mtime,\n                reverse=True,\n            )\n            if not recommend_files:\n                return None\n            recommend_file = recommend_files[0]\n    else:\n        recommend_files = sorted(\n            RECOMMEND_DIR.glob(\"recommend_*.json\"),\n            key=lambda p: p.stat().st_mtime,\n            reverse=True,\n        )\n        if not recommend_files:\n            return None\n        recommend_file = recommend_files[0]\n\n    topics_set = set()\n    try:\n        with recommend_file.open(\"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                try:\n                    rec = json.loads(line)\n                    topics = rec.get(\"topics\")\n                    if topics and isinstance(topics, list):\n                        topics_set.update(t for t in topics if t)\n                except (json.JSONDecodeError, Exception):\n                    continue\n    except Exception:\n        return None\n\n    if not topics_set:\n        return None\n\n    topics_list = sorted(list(topics_set))\n    topics_str = \", \".join(topics_list)\n\n    return (\n        \"The selected papers may also cover research areas including: \"\n        f\"{topics_str}. You may gain a broader understand of the field \"\n        \"by exploring these related research directions\"\n    )\n\n\ndef inject_paper_titles(plan: dict, papers: List[Dict[str, Any]]) -> None:\n    if not papers:\n        return\n\n    id_to_title = {p.get(\"id\"): p.get(\"title\", \"\") for p in papers if p.get(\"id\")}\n\n    reading_order = plan.get(\"reading_order\")\n    if not reading_order or not isinstance(reading_order, list):\n        return\n\n    for item in reading_order:\n        if not isinstance(item, dict):\n            continue\n        pid = item.get(\"paper_id\")\n        if pid and pid in id_to_title:\n            item[\"paper_title\"] = id_to_title[pid]\n\n\ndef make_trace_id() -> str:\n    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%S\")\n    return f\"{ts}-{uuid.uuid4().hex[:8]}\"\n\n\ndef log_jsonl(path: Path, obj: dict) -> None:\n    path.parent.mkdir(parents=True, exist_ok=True)\n    with path.open(\"a\", encoding=\"utf-8\") as f:\n        f.write(json.dumps(obj, ensure_ascii=False) + \"\n\")\n\n\ndef _normalize_text_format(fmt: Dict[str, Any]) -> Dict[str, Any]:\n    if fmt.get(\"type\") == \"json_schema\" and \"schema\" in fmt:\n        if \"name\" not in fmt:\n            fmt[\"name\"] = \"plan_schema\"\n        fmt.setdefault(\"strict\", True)\n        return fmt\n\n    if fmt.get(\"type\") == \"json_schema\" and isinstance(fmt.get(\"json_schema\"), dict):\n        js = fmt[\"json_schema\"]\n        name = js.get(\"name\") or \"plan_schema\"\n        strict = js.get(\"strict\", True)\n        schema = js.get(\"schema\")\n        if not isinstance(schema, dict):\n            raise ValueError(\"text_format.json_schema.schema must be an object\")\n        return {\n            \"type\": \"json_schema\",\n            \"name\": name,\n            \"schema\": schema,\n            \"strict\": strict,\n        }\n\n    raise ValueError(\"Unsupported text_format shape for Responses API\")\n\n\ndef call_with_retries(\n    client: OpenAI,\n    model: str,\n    messages: List[Dict[str, str]],\n    text_format: Dict[str, Any],\n    temperature: float,\n    timeout_sec: int,\n    max_retries: int,\n):\n    inputs = [\n        {\n            \"role\": m[\"role\"],\n            \"content\": [\n                {\"type\": \"input_text\", \"text\": m[\"content\"]},\n            ],\n        }\n        for m in messages\n    ]\n\n    last_err: Optional[Exception] = None\n\n    for attempt in range(1, max_retries + 1):\n        start = time.time()\n        try:\n            resp = client.responses.create(\n                model=model,\n                input=inputs,\n                temperature=temperature,\n                text={\"format\": text_format},\n                timeout=timeout_sec,\n            )\n            latency = time.time() - start\n            return resp, latency\n\n        except Exception as e:\n            last_err = e\n            if attempt == max_retries:\n                raise\n            time.sleep(2 * attempt)\n\n    raise last_err\n\n\ndef extract_text(resp) -> str:\n    try:\n        txt = getattr(resp, \"output_text\", None)\n        if txt:\n            return txt\n\n        if hasattr(resp, \"output\") and resp.output:\n            first = resp.output[0]\n            content = getattr(first, \"content\", None)\n            if content:\n                part = content[0]\n                part_text = getattr(part, \"text\", None)\n\n                if isinstance(part_text, str):\n                    return part_text\n\n                if hasattr(part_text, \"value\"):\n                    return part_text.value\n\n                if isinstance(part_text, dict):\n                    if \"value\" in part_text:\n                        return str(part_text[\"value\"])\n                    if \"text\" in part_text:\n                        return str(part_text[\"text\"])\n\n                if isinstance(part, dict) and \"text\" in part:\n                    return str(part[\"text\"])\n\n        return \"\"\n    except Exception:\n        return \"\"\n\n\ndef validate_against_schema(schema: dict, data: dict) -> None:\n    try:\n        Draft202012Validator(schema).validate(data)\n    except ValidationError as e:\n        loc = \" -> \".join([str(p) for p in e.path]) or \"<root>\"\n        raise ValueError(f\"schema fail at [{loc}]: {e.message}\") from e\n\n\ndef estimate_cost_usd(model: str, in_tokens: Optional[int], out_tokens: Optional[int]) -> Optional[float]:\n    try:\n        in_price = float(os.environ.get(\"PRICE_IN_USD_PER_1K\", \"0\"))\n        out_price = float(os.environ.get(\"PRICE_OUT_USD_PER_1K\", \"0\"))\n        if in_tokens is None or out_tokens is None:\n            return None\n        return (in_tokens / 1000.0) * in_price + (out_tokens / 1000.0) * out_price\n    except Exception:\n        return None\n\n\ndef generate_plan(\n    *,\n    model: str = os.environ.get(\"OPENAI_MODEL\", \"gpt-4.1-mini\"),\n    temperature: float = float(os.environ.get(\"OPENAI_TEMPERATURE\", \"0.2\")),\n    timeout_sec: int = int(os.environ.get(\"OPENAI_TIMEOUT\", \"60\")),\n    max_retries: int = int(os.environ.get(\"OPENAI_MAX_RETRIES\", \"5\")),\n    schema_path: Optional[Path] = None,\n    papers_path: Optional[Path] = None,\n) -> dict:\n    schema_path = schema_path or max(DATA_DIR.glob(\"plan_schema_*.json\"), key=lambda f: f.stat().st_mtime)\n    papers_path = papers_path or max(DATA_DIR.glob(\"standardize_input_*.json\"), key=lambda f: f.stat().st_mtime)\n\n    schema = load_plan_schema(str(schema_path))\n    papers = read_papers_jsonl(papers_path)\n    if not papers:\n        raise ValueError(\"no papers\")\n\n    goal = pick_goal_from_query(papers)\n\n    messages = build_messages(\n        goal=goal,\n        papers=papers,\n        study_level_hint=None,\n        max_abstract_chars=800,\n    )\n    text_format = _normalize_text_format(build_response_format(schema))\n\n    client = OpenAI()\n    trace_id = make_trace_id()\n\n    resp, latency = call_with_retries(\n        client=client,\n        model=model,\n        messages=messages,\n        text_format=text_format,\n        temperature=temperature,\n        timeout_sec=timeout_sec,\n        max_retries=max_retries,\n    )\n\n    text = extract_text(resp).strip()\n    try:\n        plan = json.loads(text)\n    except json.JSONDecodeError as e:\n        debug_path = DATA_DIR / \"artifacts\" / f\"bad_output_{trace_id}.txt\"\n        debug_path.write_text(text, encoding=\"utf-8\")\n        raise ValueError(f\"model did not return valid JSON (see {debug_path})\") from e\n\n    all_ids: List[str] = [p.get(\"id\") for p in papers if p.get(\"id\")]\n    plan[\"source_papers\"] = all_ids\n\n    meta = plan.get(\"metadata\") or {}\n    meta[\"prompt_version\"] = PROMPT_VERSION\n    meta[\"model\"] = model\n    meta[\"created_at\"] = datetime.now(timezone.utc).isoformat()\n    plan[\"metadata\"] = meta\n\n    validate_against_schema(schema, plan)\n    assert_reading_order_from_source(plan)\n\n    inject_paper_titles(plan, papers)\n\n    topics_suggestion = collect_topics_from_recommend(prefer_view=\"default\")\n    if topics_suggestion:\n        plan[\"_topics_suggestion\"] = topics_suggestion\n\n    artifacts = DATA_DIR / \"artifacts\"\n    artifacts.mkdir(parents=True, exist_ok=True)\n    out_path = artifacts / f\"plan_{trace_id}.json\"\n    out_path.write_text(json.dumps(plan, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n\n    latest_path = DATA_DIR / \"plan_latest.json\"\n    latest_path.write_text(json.dumps(plan, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n\n    usage = getattr(resp, \"usage\", None)\n    in_tokens = None\n    out_tokens = None\n    if usage is not None:\n        in_tokens = getattr(usage, \"input_tokens\", None) or getattr(usage, \"prompt_tokens\", None)\n        out_tokens = getattr(usage, \"output_tokens\", None) or getattr(usage, \"completion_tokens\", None)\n\n    cost = estimate_cost_usd(model, in_tokens, out_tokens)\n\n    log_obj = {\n        \"trace_id\": trace_id,\n        \"ts_utc\": datetime.now(timezone.utc).isoformat(),\n        \"prompt_version\": PROMPT_VERSION,\n        \"model\": model,\n        \"temperature\": temperature,\n        \"latency_sec\": round(latency, 3) if latency is not None else None,\n        \"tokens_in\": in_tokens,\n        \"tokens_out\": out_tokens,\n        \"cost_usd_estimate\": cost,\n        \"artifact\": str(out_path),\n        \"schema_file\": str(schema_path),\n        \"papers_file\": str(papers_path),\n        \"status\": \"ok\",\n    }\n\n    log_dir = DATA_DIR / \"logs\"\n    log_dir.mkdir(parents=True, exist_ok=True)\n    log_path = log_dir / f\"inference_{datetime.now(UTC).strftime('%Y-%m-%d_%H%M')}.json\"\n    log_jsonl(log_path, log_obj)\n\n    print(\"\n== Run Summary ==\")\n    print(f\"trace_id: {trace_id}\")\n    print(f\"model: {model}  temp: {temperature}\")\n    print(\n        f\"latency: {round(latency, 2) if latency is not None else 'n/a'}s  \"\n        f\"tokens(in/out): {in_tokens}/{out_tokens}  cost~: {cost}\"\n    )\n    print(f\"artifact: {out_path}\")\n    print(f\"latest:   {latest_path}\")\n\n    return plan\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Example usage\nThe cells above can be run individually. A minimal happy path is:\n1. `build_selected_papers()` to create a fresh `standardize_input_*.json`.\n2. `schema_path = write_schema()` to emit the latest schema.\n3. `plan = generate_plan(schema_path=schema_path)` to call OpenAI and store artifacts/logs.\n\nMake sure `OPENAI_API_KEY` (and optional pricing env vars) are set in the environment before running step 3.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}