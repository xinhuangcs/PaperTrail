{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "935270ef76c7840",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipeline  \n",
    "This Notebook documents the complete data preprocessing workflow.     \n",
    "  \n",
    "The project uses the [arXiv Dataset](https://www.kaggle.com/Cornell-University/arxiv). We also provide the [processed dataset](https://github.com/xinhuangcs/PaperTrail/releases) for direct use.\n",
    "  \n",
    "1. **Original Size**: ~2.84M papers.\n",
    "2. **Filtering**: We filter for categories starting with `cs.` (Computer Science), `stat.` (Statistics), and `eess.` (Electrical Engineering and Systems Science), etc.\n",
    "3. **Add citation data**: Use the OpenAlex API to add citation data to papers.\n",
    "4. **Final Dataset**: Contains ~730,000 papers used for the search index.\n",
    "5. **Preprocessing**: Text cleaning, stop word removal, and stemming are applied before indexing.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Our dataset was downloaded on September 26, 2025, and the statistics are based on this date.  \n",
    "When we expanded our citation data, we discovered that OpenAlex has not yet indexed some publications released since 2025. Therefore, our dataset currently only includes papers published up to the end of 2024.  \n",
    "  \n",
    "The core logic has been implemented in Python scripts within the `src/preprocess_data/` directory.    \n",
    "Because of the large size of the source code, this Notebook serves as the scheduling layer, executing these scripts sequentially while explaining the logic behind each step.  \n",
    "  \n",
    "**⏱️ Estimated runtime (excluding data-download time):** more than 200 Hours  \n",
    "  * **\\>200 Hours**: Get the newest original [ArXiv Dataset](https://www.kaggle.com/Cornell-University/arxiv) and call OpenAlex API to insert citation data.    \n",
    "   * **\\<1 Hours**: For data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eafa39de1919769",
   "metadata": {},
   "source": [
    "**1. Environment & Path Setup**\n",
    " **Prerequisites**:    \n",
    "- Python 3.12+  \n",
    "- When running the Notebook, ensure that the current working directory is `src/jupyter_notebook/` , in order to get the relative paths used below correct.\n",
    "- **Download Data**: Get the newest original [ArXiv Dataset](https://www.kaggle.com/Cornell-University/arxiv) and place `arxiv-metadata-oai-snapshot.json` in `data/preprocess/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e9896f8b65792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7532d116f18d7a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:16:58.649031Z",
     "start_time": "2025-12-03T20:16:57.871198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r ../requirements.txt (line 3)) (2.6.1)\r\n",
      "Requirement already satisfied: jsonschema>=4.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r ../requirements.txt (line 4)) (4.25.1)\r\n",
      "Requirement already satisfied: tqdm>=4.60.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r ../requirements.txt (line 5)) (4.66.4)\r\n",
      "Requirement already satisfied: numpy>=1.24.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r ../requirements.txt (line 6)) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r ../requirements.txt (line 7)) (1.13.1)\r\n",
      "Requirement already satisfied: scikit-learn>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r ../requirements.txt (line 8)) (1.7.2)\r\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from -r ../requirements.txt (line 9)) (1.2.1)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.0.0->-r ../requirements.txt (line 3)) (4.2.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.0.0->-r ../requirements.txt (line 3)) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.0.0->-r ../requirements.txt (line 3)) (0.27.0)\r\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.0.0->-r ../requirements.txt (line 3)) (0.11.1)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.0.0->-r ../requirements.txt (line 3)) (2.10.3)\r\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.0.0->-r ../requirements.txt (line 3)) (1.3.0)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.0.0->-r ../requirements.txt (line 3)) (4.12.2)\r\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai>=1.0.0->-r ../requirements.txt (line 3)) (3.7)\r\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.0.0->-r ../requirements.txt (line 3)) (2024.7.4)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.0.0->-r ../requirements.txt (line 3)) (1.0.2)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0->-r ../requirements.txt (line 3)) (0.14.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->-r ../requirements.txt (line 3)) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai>=1.0.0->-r ../requirements.txt (line 3)) (2.27.1)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.0.0->-r ../requirements.txt (line 4)) (23.1.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.0.0->-r ../requirements.txt (line 4)) (2023.7.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.0.0->-r ../requirements.txt (line 4)) (0.30.2)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.0.0->-r ../requirements.txt (line 4)) (0.10.6)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=1.4.0->-r ../requirements.txt (line 8)) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn>=1.4.0->-r ../requirements.txt (line 8)) (3.6.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940bab99f33150f7",
   "metadata": {},
   "source": [
    "**2. Reduce 2.8M papers to a manageable subset relevant to CS** (src/preprocess_data/0_1_reduce_categories.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4086f6ac4e20831",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:17:18.529297Z",
     "start_time": "2025-12-03T20:17:02.327755Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "过滤中: 100%|██████████| 2840638/2840638 [00:12<00:00, 226262.27line/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Summary ===\n",
      "Input:     /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-metadata-oai-snapshot.json\n",
      "Output:    /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data.json\n",
      "Read:      2,840,638\n",
      "Kept:      895,809\n",
      "Skipped:   1,944,829\n",
      "By category:\n",
      "  - Computer Science: 1,238,160\n",
      "  - Statistics: 163,543\n",
      "  - EESS: 116,936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run ../src/preprocess_data/0_1_reduce_categories.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9865a17aa2fb6534",
   "metadata": {},
   "source": [
    " **3. Add citation counts from OpenAlex using DOI and Title matching (sliced execution)**(`(1_0_add_incite_num)`)  \n",
    "We enriched the dataset with citation counts by querying the OpenAlex API using DOIs, falling back to vague title matching for entries without identifiers. \n",
    "\n",
    "\n",
    "**Due to the prohibitive runtime of over 200 hours for the full dataset,Here, we manually constructed a minimal dataset to test whether Jupyter can run.**  \n",
    "\n",
    "\n",
    "The processed files can be get from the releases section of our GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55ee250a78f4ffa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:20:01.848462Z",
     "start_time": "2025-12-03T20:19:56.991522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Parameters:\n",
      "    INPUT_FILE  = /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data.json\n",
      "    OUTPUT_FILE = /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data-with-citations_slice6.jsonl\n",
      "    CACHE_FILE  = /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/citation_cache.json\n",
      "    SLICE       = 6/19\n",
      "    SLEEP_SECS  = 0.2\n",
      "[i] Cache loaded: 0 entries\n",
      "[i] Total lines=895809, this slice range=268741 ~ 313530 (total 44790 lines)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Slice 6/19:   0%|          | 7/44790 [00:03<5:55:15,  2.10rec/s] \n",
      " Interrupted, saving cache\n",
      "Slice 6/19:   0%|          | 8/44790 [00:04<6:39:57,  1.87rec/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed: processed=8, written=8\n",
      "Output file: /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data-with-citations_slice6.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run ../src/preprocess_data/1_0_add_incite_num.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae068531eb67b3a",
   "metadata": {},
   "source": [
    "**4. Merge distributed processing slices into a unified dataset** (`(1_1_merge_slices)`)  \n",
    "\n",
    "\n",
    "We used this script to merge multiple dispersed JSONL slice files into a single final dataset file. By streaming the read and write operations line-by-line, we efficiently consolidate the data while minimizing memory usage.\n",
    "\n",
    "\n",
    "Here, we manually constructed a minimal dataset to test whether Jupyter can run.  \n",
    "\n",
    "\n",
    "The processed files can be get from the releases section of our GitHub repository.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97cc6b812eeb0aee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:20:35.641441Z",
     "start_time": "2025-12-03T20:20:35.635945Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging (any order): 100%|██████████| 1/1 [00:00<00:00, 1855.07file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge done → /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data-with-citations-final-dataset.json\n",
      "[i] Files: 1 | Lines: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run ../src/preprocess_data/1_1_merge_slices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd6910ff55c76a",
   "metadata": {},
   "source": [
    "**5. Fix missing citation data using a multi-stage fallback strategy (Truncated Title/Abstract search)** (`(1_2_retry_record_of_negone)`)  \n",
    "We addressed records with missing citation data by implementing a multi-stage fallback strategy that iteratively queries OpenAlex using DOIs, truncated titles, and abstract snippets. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Here, we manually constructed a minimal dataset to test whether Jupyter can run.  \n",
    "\n",
    "\n",
    "The processed files can be get from the releases section of our GitHub repository.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ba916ace6c75364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:22:19.960629Z",
     "start_time": "2025-12-03T20:22:16.735089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Parameters:\n",
      "    INPUT_FILE   = /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data-with-citations_merged_zrk_5.json\n",
      "    OUTPUT_FILE  = /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data-with-citations_final_dataset_odd.json\n",
      "    CACHE_FILE   = /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/citation_cache.json\n",
      "    SLEEP_SECS   = 0.25\n",
      "    TITLE_SIM    = 0.8\n",
      "[i] Cache loaded: 8 entries\n",
      "[i] Records to re-check (citation_count == -1): 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Re-check (-1 only): 100%|██████████| 4/4 [00:03<00:00,  1.25rec/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match summary:\n",
      "  DOI:     attempt=0, ok=0, nf=0\n",
      "  Title:   attempt=4, ok=0, nf=4\n",
      "  Title4:  attempt=4, ok=3, nf=1, skipped=0\n",
      "  Abstract:attempt=1, ok=0, nf=1, skipped=0\n",
      "  Final:   ok=3, neg1=1\n",
      "Second pass done. -1 processed=4, total written=96\n",
      "Output file: /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data-with-citations_final_dataset_odd.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run ../src/preprocess_data/1_2_retry_record_of_negone.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6ce62aea4ef76b",
   "metadata": {},
   "source": [
    "**6. Normalize text data and prepare `processed_content` for embedding/indexing** (`(2_data_filtering)`)  \n",
    "\n",
    "\n",
    "\n",
    "We implemented a text normalization pipeline designed to standardize unstructured paper metadata by merging titles and abstracts into a single textual feature. It applies a functional transformations, such as lowercasing, noise removal, and Porter stemming—executed.\n",
    "\n",
    "Here, we manually constructed a minimal dataset to test whether Jupyter can run.  \n",
    "\n",
    "\n",
    "The processed files can be get from the releases section of our GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a317e322b566d86c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:25:50.099187Z",
     "start_time": "2025-12-03T20:25:46.544039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch processing for large dataset...\r\n",
      "Processing large dataset in batches of 1000...\r\n",
      "Completed! Total processed: 8 papers\r\n",
      "Saved to: /Users/jasonh/Desktop/02807/PaperTrail/data/preprocess/arxiv-cs-data-with-citations-final-dataset_preprocessed.json\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} ../src/preprocess_data/2_data_filtering.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04e07c35985c873",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
