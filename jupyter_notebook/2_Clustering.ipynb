{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ef1a31",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "This jupyter notebook aims to build a high-precision document clustering analysis system. First, through a “dual-validation” strategy combining Scikit-learn standard libraries with manual implementation, we established a mathematically precise and equivalent TF-IDF vectorization model. We then employed Latent LSA to reduce the dimensionality and remove noise from high-dimensional sparse matrices. Combined with the MiniBatch K-Means algorithm, this enabled efficient unsupervised clustering and topic segmentation of massive document datasets.\n",
    "\n",
    "\n",
    "To further uncover deep semantic relationships, the system incorporates Sentence Transformer-based semantic encoding and UMAP manifold dimension reduction techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a99f85",
   "metadata": {},
   "source": [
    "# TF-IDF Clustering\n",
    "\n",
    "- This module aims to construct a document vectorization model based on TF-IDF to provide core support for downstream similarity retrieval tasks. In order to ensure the accuracy and controllability of the algorithm implementation, the system adopts a “two-way implementation” strategy, i.e., while using the Scikit-learn standard library for production-level construction, it maintains a set of manually-implemented MyTfidfVectorizer for algorithmic logic validation, and both of them ensure the precise equivalence of the mathematical expressions through a strict consistency checking mechanism. The two ensure the precise equivalence of mathematical expressions through a strict consistency checking mechanism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8ecadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Set up root directory\n",
    "ROOT_DIR = Path(\"..\").resolve()\n",
    "if str(ROOT_DIR) not in sys.path:\n",
    "    sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "# Add src/tf_idf to sys.path to allow importing local modules like my_tfidf\n",
    "SRC_TFIDF_DIR = ROOT_DIR / \"src\" / \"tf_idf\"\n",
    "if str(SRC_TFIDF_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_TFIDF_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aeb7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from pathlib import Path\n",
    "#Config\n",
    "# ROOT_DIR = Path(__file__).resolve().parents[2]  # Modified for notebook\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = Path('..').resolve()\n",
    "LSA_INPUT_PATH = ROOT_DIR / \"data\" / \"lsa\" / \"lsa_reduced.npz\"\n",
    "CLUSTER_LABELS_PATH = ROOT_DIR / \"data\" / \"lsa\" / \"cluster_labels.npy\"\n",
    "\n",
    "K_FIXED = 40\n",
    "\n",
    "def main():\n",
    "    # 1) Load LSA reduced matrix\n",
    "    data = np.load(LSA_INPUT_PATH)\n",
    "    X = data['X_reduced']\n",
    "    print(f\"[i] Loaded LSA matrix: shape={X.shape}\")\n",
    "\n",
    "    print(f\"[i] Running MiniBatchKMeans with fixed K={K_FIXED}\")\n",
    "    kmeans = MiniBatchKMeans(n_clusters=K_FIXED, init='k-means++', n_init=10, random_state=42)\n",
    "    final_labels = kmeans.fit_predict(X)\n",
    "\n",
    "    inertia = kmeans.inertia_\n",
    "    silhouette = silhouette_score(X, final_labels)\n",
    "    print(f\"[i] Inertia={inertia:.2f}, Silhouette={silhouette:.4f}\")\n",
    "\n",
    "    np.save(CLUSTER_LABELS_PATH, final_labels)\n",
    "    print(f\"[i] Cluster labels saved to: {CLUSTER_LABELS_PATH}\")\n",
    "\n",
    "    # 4)Output Final clustering results:\n",
    "    unique, counts = np.unique(final_labels, return_counts=True)\n",
    "    print(\"Final clustering results:\")\n",
    "    for cid, count in zip(unique, counts):\n",
    "        print(f\" - Cluster {cid}: {count} papers\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f39c94",
   "metadata": {},
   "source": [
    "## Build TF-IDF (Standard)\n",
    "\n",
    "- The data pipeline takes the pre-processed text (title and abstract) in JSONL format as input, and performs feature extraction with uniform configuration parameters. The system limits the vocabulary size to 100,000 and enables a combination of Unigram and Bigram to capture phrase-level features. In terms of noise control, the module loads a customized deactivated word list, and uses min_df=5 and max_df=0.8 to dynamically remove very low-frequency long-tailed noise and too high-frequency general-purpose words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abb8da5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Loaded 465 custom stopwords from /work3/s242644/ds/PaperTrail/src/custom_stopwords.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL: 732367it [00:14, 51120.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Loaded documents: 732,367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/s242644/seq_po_d1/seq_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algorithm', 'attack', 'best', 'first', 'llm', 'number', 'second'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] TF-IDF shape: (732367, 100000), nnz=72,361,100, dtype=float32, type=<class 'scipy.sparse._csr.csr_matrix'>\n",
      "[i] Saved: /work3/s242644/ds/PaperTrail/data/tf_idf/tfidf_matrix.npz\n",
      "[i] Saved: /work3/s242644/ds/PaperTrail/data/tf_idf/tfidf_vectorizer.joblib\n",
      "[i] Saved: /work3/s242644/ds/PaperTrail/data/tf_idf/doc_ids.npy, /work3/s242644/ds/PaperTrail/data/tf_idf/doc_titles.npy\n",
      "Doc#0 (2107.12674) 'Vision-Guided Forecasting -- Visual Context for Multi-Horizon Time\n",
      "  Series Forecasting' top terms: [('forecast', 0.1998753696680069), ('horizon', 0.18677020072937012), ('vehicl', 0.16268596053123474), ('visual multi', 0.1325095146894455), ('face camera', 0.13044185936450958)]\n",
      "Doc#1 (2107.12675) 'Feature Fusion Methods for Indexing and Retrieval of Biometric Data:\n",
      "  Application to Face Recognition with Privacy Protection' top terms: [('biometr', 0.25766581296920776), ('biometr identif', 0.24242226779460907), ('templat', 0.19307062029838562), ('protect', 0.16008898615837097), ('homomorph encrypt', 0.1581457257270813)]\n",
      "Doc#2 (2107.12676) 'QoS-aware User Grouping Strategy for Downlink Multi-Cell NOMA Systems' top terms: [('group strategi', 0.26320210099220276), ('inter cell', 0.18499839305877686), ('cell interfer', 0.184128999710083), ('qo', 0.1783108115196228), ('multi cell', 0.1779417246580124)]\n",
      "Doc#3 (2107.12677) 'Deep Variational Models for Collaborative Filtering-based Recommender\n",
      "  Systems' top terms: [('collabor filter', 0.2400057464838028), ('collabor', 0.17331692576408386), ('variat', 0.15856754779815674), ('provid accur', 0.15700401365756989), ('variat autoencod', 0.15172559022903442)]\n",
      "Doc#4 (2107.12679) 'MFAGAN: A Compression Framework for Memory-Efficient On-Device\n",
      "  Super-Resolution GAN' top terms: [('textbf', 0.22587668895721436), ('base sr', 0.18527358770370483), ('sr', 0.16930608451366425), ('gan', 0.16806969046592712), ('gan base', 0.14357168972492218)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build TF-IDF sparse matrix + vectorizer; save to disk; also export doc index -> (id, title) mapping.\n",
    "\n",
    "dependency:\n",
    "  pip install scikit-learn scipy joblib tqdm\n",
    "\n",
    "Input:\n",
    "  A JSONL file. Each line is one paper with field \"processed_content\" (title+abstract, already cleaned).\n",
    "\n",
    "Output:\n",
    "  - tfidf_matrix.npz: scipy csr_matrix\n",
    "  - tfidf_vectorizer.joblib: Trained TfidfVectorizer, use for future transform/query\n",
    "  - doc_ids.npy / doc_titles.npy: Arrays of IDs / titles corresponding to each row of the matrix\n",
    "  - Terminal output: Scale information, non-zero feature statistics for the first few documents\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "#Config\n",
    "\n",
    "# ROOT_DIR = Path(__file__).resolve().parents[2]  # Modified for notebook\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = Path('..').resolve()\n",
    "INPUT_JSONL = ROOT_DIR / \"data\" / \"preprocess\" / \"arxiv-cs-data-with-citations-final-dataset_preprocessed.json\"\n",
    "OUT_DIR = ROOT_DIR / \"data\" / \"tf_idf\"\n",
    "TFIDF_NPZ_PATH = OUT_DIR / \"tfidf_matrix.npz\"\n",
    "VECTORIZER_PKL_PATH = OUT_DIR / \"tfidf_vectorizer.joblib\"\n",
    "DOC_IDS_NPY = OUT_DIR / \"doc_ids.npy\"\n",
    "DOC_TITLES_NPY = OUT_DIR / \"doc_titles.npy\"\n",
    "CUSTOM_STOPWORDS_PATH = ROOT_DIR / \"src\" / \"custom_stopwords.txt\"\n",
    "\n",
    "# Recommended params for ~900k CS papers (tune if needed)\n",
    "def load_custom_stopwords(path: Path) -> list[str]:\n",
    "    if not path.exists():\n",
    "        print(f\"[warn] Custom stopword file not found: {path}\")\n",
    "        return []\n",
    "    words = set()\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            token = line.strip()\n",
    "            if not token or token.startswith(\"#\"):\n",
    "                continue\n",
    "            words.add(token)\n",
    "    print(f\"[i] Loaded {len(words)} custom stopwords from {path}\")\n",
    "    return sorted(words)\n",
    "\n",
    "\n",
    "VECTORIZER_KW = dict(\n",
    "    max_df=0.8,        #drop terms in >=80% docs\n",
    "    min_df=5,         #drop terms in <5 docs\n",
    "    max_features=100_000,  #cap vocab size to control memory\n",
    "    ngram_range=(1, 2),    # # Only unigrams; change to (1,2) for phrases (significantly increases scale)\n",
    "    sublinear_tf=True,     # log/sublinear TF scaling\n",
    "    norm=\"l2\",             # \n",
    "    dtype=np.float32,      # halve memory vs float64\n",
    "    stop_words=None,       # Data has been cleaned and stemmed; no additional universal stop words are applied here\n",
    "    lowercase=False,       # processed_content already lower\n",
    ")\n",
    "\n",
    "# Sample inspection\n",
    "PRINT_TOP_N_DOCS = 5       # inspect first N docs\n",
    "TOP_TERMS_PER_DOC = 5     # show top-K terms per doc\n",
    "\n",
    "\n",
    "def read_corpus_and_meta(jsonl_path):\n",
    "    \"\"\"\n",
    "    Read corpus & metadata: return (texts, ids, titles)\n",
    "    \"\"\"\n",
    "    texts, ids, titles = [], [], []\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"Reading JSONL\"):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # print(\"[warn] malformed JSON line skipped\")\n",
    "                continue\n",
    "            # --------------------------------------------------------------\n",
    "            text = rec.get(\"processed_content\") or \"\"\n",
    "            paper_id = rec.get(\"id\") or \"\"\n",
    "            title = rec.get(\"title\") or \"\"\n",
    "            texts.append(text)\n",
    "            ids.append(paper_id)\n",
    "            titles.append(title)\n",
    "    return texts, np.array(ids), np.array(titles)\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    custom_stopwords = load_custom_stopwords(CUSTOM_STOPWORDS_PATH)\n",
    "    if custom_stopwords:\n",
    "        VECTORIZER_KW[\"stop_words\"] = custom_stopwords\n",
    "    # 1) Read texts & metadata\n",
    "    texts, ids, titles = read_corpus_and_meta(INPUT_JSONL)\n",
    "    print(f\"[i] Loaded documents: {len(texts):,}\")\n",
    "    # 2)TF-IDF:Fit + transform\n",
    "    vectorizer = TfidfVectorizer(**VECTORIZER_KW)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    print(f\"[i] TF-IDF shape: {X.shape}, nnz={X.nnz:,}, dtype={X.dtype}, type={type(X)}\")\n",
    "    # 3)Persist to disk\n",
    "    sparse.save_npz(TFIDF_NPZ_PATH, X)\n",
    "    joblib.dump(vectorizer, VECTORIZER_PKL_PATH)\n",
    "    np.save(DOC_IDS_NPY, ids)\n",
    "    np.save(DOC_TITLES_NPY, titles)\n",
    "    print(f\"[i] Saved: {TFIDF_NPZ_PATH}\")\n",
    "    print(f\"[i] Saved: {VECTORIZER_PKL_PATH}\")\n",
    "    print(f\"[i] Saved: {DOC_IDS_NPY}, {DOC_TITLES_NPY}\")\n",
    "    #4 Quick sanity check: show top terms for first few docs\n",
    "    feats = vectorizer.get_feature_names_out()\n",
    "    for di in range(min(PRINT_TOP_N_DOCS, X.shape[0])):\n",
    "        row = X[di]\n",
    "        idx = row.indices\n",
    "        val = row.data\n",
    "        if val.size == 0:\n",
    "            print(f\"Doc#{di} ({ids[di]}): <empty vector>\")\n",
    "            continue\n",
    "        order = np.argsort(-val)[:TOP_TERMS_PER_DOC]\n",
    "        top_terms = [(feats[idx[j]], float(val[j])) for j in order]\n",
    "        print(f\"Doc#{di} ({ids[di]}) '{titles[di]}' top terms:\", top_terms)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0ff54",
   "metadata": {},
   "source": [
    "## Build TF-IDF (Manual)\n",
    "- The data pipeline takes the pre-processed text (title and abstract) in JSONL format as input, and performs feature extraction with uniform configuration parameters. The system limits the vocabulary size to 100,000 and enables a combination of Unigram and Bigram to capture phrase-level features. In terms of noise control, the module loads a customized deactivated word list, and uses min_df=5 and max_df=0.8 to dynamically remove very low-frequency long-tailed noise and too high-frequency general-purpose words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d24b7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Loaded 465 custom stopwords from /work3/s242644/ds/PaperTrail/src/custom_stopwords.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL: 732367it [00:14, 51484.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Loaded documents: 732,367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/s242644/seq_po_d1/seq_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algorithm', 'attack', 'best', 'first', 'llm', 'number', 'second'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] TF-IDF shape: (732367, 100000), nnz=72,361,100, dtype=float64, type=<class 'scipy.sparse._csr.csr_matrix'>\n",
      "[i] Saved: /work3/s242644/ds/PaperTrail/data/tf_idf_manual/tfidf_matrix.npz\n",
      "[i] Saved: /work3/s242644/ds/PaperTrail/data/tf_idf_manual/tfidf_vectorizer.joblib\n",
      "[i] Saved: /work3/s242644/ds/PaperTrail/data/tf_idf_manual/doc_ids.npy, /work3/s242644/ds/PaperTrail/data/tf_idf_manual/doc_titles.npy\n",
      "Doc#0 (2107.12674) 'Vision-Guided Forecasting -- Visual Context for Multi-Horizon Time\n",
      "  Series Forecasting' top terms: [('forecast', 0.19987536629425323), ('horizon', 0.18677019627576547), ('vehicl', 0.16268595269368802), ('visual multi', 0.13250951186926718), ('face camera', 0.1304418637540388)]\n",
      "Doc#1 (2107.12675) 'Feature Fusion Methods for Indexing and Retrieval of Biometric Data:\n",
      "  Application to Face Recognition with Privacy Protection' top terms: [('biometr', 0.2576658227490619), ('biometr identif', 0.2424222594275143), ('templat', 0.19307061494616037), ('protect', 0.16008897706957764), ('homomorph encrypt', 0.1581457331013303)]\n",
      "Doc#2 (2107.12676) 'QoS-aware User Grouping Strategy for Downlink Multi-Cell NOMA Systems' top terms: [('group strategi', 0.2632020919159408), ('inter cell', 0.18499837537207336), ('cell interfer', 0.18412902278536322), ('qo', 0.1783108133996266), ('multi cell', 0.17794173380783188)]\n",
      "Doc#3 (2107.12677) 'Deep Variational Models for Collaborative Filtering-based Recommender\n",
      "  Systems' top terms: [('collabor filter', 0.24000574896222981), ('collabor', 0.1733169259033827), ('variat', 0.158567542100537), ('provid accur', 0.1570040214264456), ('variat autoencod', 0.15172559357412865)]\n",
      "Doc#4 (2107.12679) 'MFAGAN: A Compression Framework for Memory-Efficient On-Device\n",
      "  Super-Resolution GAN' top terms: [('textbf', 0.22587668960799517), ('base sr', 0.185273590734354), ('sr', 0.16930608774172679), ('gan', 0.16806969672856256), ('gan base', 0.1435716873732708)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build TF-IDF sparse matrix + vectorizer; save to disk; also export doc index -> (id, title) mapping.\n",
    "\n",
    "dependency:\n",
    "  pip install scikit-learn scipy joblib tqdm\n",
    "\n",
    "Input:\n",
    "  A JSONL file. Each line is one paper with field \"processed_content\" (title+abstract, already cleaned).\n",
    "\n",
    "Output:\n",
    "  - tfidf_matrix.npz: scipy csr_matrix\n",
    "  - tfidf_vectorizer.joblib: Trained TfidfVectorizer, use for future transform/query\n",
    "  - doc_ids.npy / doc_titles.npy: Arrays of IDs / titles corresponding to each row of the matrix\n",
    "  - Terminal output: Scale information, non-zero feature statistics for the first few documents\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from scipy import sparse\n",
    "from my_tfidf import MyTfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "# 1) Config\n",
    "# ROOT_DIR = Path(__file__).resolve().parents[2]  # Modified for notebook\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = Path('..').resolve()\n",
    "\n",
    "INPUT_JSONL = ROOT_DIR / \"data\" / \"preprocess\" / \"arxiv-cs-data-with-citations-final-dataset_preprocessed.json\"\n",
    "\n",
    "OUT_DIR = ROOT_DIR / \"data\" / \"tf_idf_manual\"\n",
    "TFIDF_NPZ_PATH = OUT_DIR / \"tfidf_matrix.npz\"\n",
    "VECTORIZER_PKL_PATH = OUT_DIR / \"tfidf_vectorizer.joblib\"\n",
    "DOC_IDS_NPY = OUT_DIR / \"doc_ids.npy\"\n",
    "DOC_TITLES_NPY = OUT_DIR / \"doc_titles.npy\"\n",
    "CUSTOM_STOPWORDS_PATH = ROOT_DIR / \"src\" / \"custom_stopwords.txt\"\n",
    "\n",
    "# Recommended params for ~900k CS papers (tune if needed)\n",
    "def load_custom_stopwords(path: Path) -> list[str]:\n",
    "    if not path.exists():\n",
    "        print(f\"[warn] Custom stopword file not found: {path}\")\n",
    "        return []\n",
    "    words = set()\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            token = line.strip()\n",
    "            if not token or token.startswith(\"#\"):\n",
    "                continue\n",
    "            words.add(token)\n",
    "    print(f\"[i] Loaded {len(words)} custom stopwords from {path}\")\n",
    "    return sorted(words)\n",
    "\n",
    "\n",
    "VECTORIZER_KW = dict(\n",
    "    max_df=0.8,        #drop terms in >=80% docs\n",
    "    min_df=5,         #drop terms in <5 docs\n",
    "    max_features=100_000,  #cap vocab size to control memory\n",
    "    ngram_range=(1, 2),    # # Only unigrams; change to (1,2) for phrases (significantly increases scale)\n",
    "    sublinear_tf=True,     # log/sublinear TF scaling\n",
    "    norm=\"l2\",             \n",
    "    dtype=np.float32,      # halve memory vs float64\n",
    "    stop_words=None,       # Data has been cleaned and stemmed; no additional universal stop words are applied here\n",
    "    lowercase=False,       # processed_content already lower\n",
    ")\n",
    "\n",
    "# Sample inspection\n",
    "PRINT_TOP_N_DOCS = 5       # inspect first N docs\n",
    "TOP_TERMS_PER_DOC = 5     # show top-K terms per doc\n",
    "\n",
    "\n",
    "def read_corpus_and_meta(jsonl_path):\n",
    "    \"\"\"\n",
    "    Read corpus & metadata: return (texts, ids, titles)\n",
    "    \"\"\"\n",
    "    texts, ids, titles = [], [], []\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"Reading JSONL\"):\n",
    "          \n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "               \n",
    "                continue\n",
    "           \n",
    "            text = rec.get(\"processed_content\") or \"\"\n",
    "            paper_id = rec.get(\"id\") or \"\"\n",
    "            title = rec.get(\"title\") or \"\"\n",
    "            texts.append(text)\n",
    "            ids.append(paper_id)\n",
    "            titles.append(title)\n",
    "    return texts, np.array(ids), np.array(titles)\n",
    "\n",
    "\n",
    "def main():\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    custom_stopwords = load_custom_stopwords(CUSTOM_STOPWORDS_PATH)\n",
    "    if custom_stopwords:\n",
    "        VECTORIZER_KW[\"stop_words\"] = custom_stopwords\n",
    "    # 1) Read texts & metadata\n",
    "    texts, ids, titles = read_corpus_and_meta(INPUT_JSONL)\n",
    "    print(f\"[i] Loaded documents: {len(texts):,}\")\n",
    "    # 2)TF-IDF:Fit + transform\n",
    "    vectorizer = MyTfidfVectorizer(**VECTORIZER_KW)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    print(f\"[i] TF-IDF shape: {X.shape}, nnz={X.nnz:,}, dtype={X.dtype}, type={type(X)}\")\n",
    "    # 3)Persist to disk\n",
    "    sparse.save_npz(TFIDF_NPZ_PATH, X)\n",
    "    joblib.dump(vectorizer, VECTORIZER_PKL_PATH)\n",
    "    np.save(DOC_IDS_NPY, ids)\n",
    "    np.save(DOC_TITLES_NPY, titles)\n",
    "    print(f\"[i] Saved: {TFIDF_NPZ_PATH}\")\n",
    "    print(f\"[i] Saved: {VECTORIZER_PKL_PATH}\")\n",
    "    print(f\"[i] Saved: {DOC_IDS_NPY}, {DOC_TITLES_NPY}\")\n",
    "    #4 Quick sanity check: show top terms for first few docs\n",
    "    feats = vectorizer.get_feature_names_out()\n",
    "    for di in range(min(PRINT_TOP_N_DOCS, X.shape[0])):\n",
    "        row = X[di]\n",
    "        idx = row.indices\n",
    "        val = row.data\n",
    "        if val.size == 0:\n",
    "            print(f\"Doc#{di} ({ids[di]}): <empty vector>\")\n",
    "            continue\n",
    "        order = np.argsort(-val)[:TOP_TERMS_PER_DOC]\n",
    "        top_terms = [(feats[idx[j]], float(val[j])) for j in order]\n",
    "        print(f\"Doc#{di} ({ids[di]}) '{titles[di]}' top terms:\", top_terms)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feca0b7",
   "metadata": {},
   "source": [
    "## Validation\n",
    "- In order to verify the correctness of the manually implemented logic, the script compares the CSR sparse matrices, serialized Vectorizer objects, and metadata indexes generated by the manually implemented version with those generated by the native version of Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8af4f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Comparing directories ---\n",
      "A: /work3/s242644/ds/PaperTrail/data/tf_idf\n",
      "B: /work3/s242644/ds/PaperTrail/data/tf_idf_manual\n",
      "\n",
      "[1] compare tfidf_matrix.npz...\n",
      "   tfidf_matrix.npz is consistent.\n",
      "\n",
      "[2] compare tfidf_vectorizer.joblib...\n",
      "  tfidf_vectorizer.joblib is consistent.\n",
      "\n",
      "[3] compare doc_ids.npy...\n",
      "   doc_ids.npy is consistent.\n",
      "\n",
      "[4] compare doc_titles.npy...\n",
      "   doc_titles.npy is consistent.\n",
      "\n",
      "==============================\n",
      "--- summary ---\n",
      " all TF-IDF related files in both directories are consistent.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import joblib\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "# 1) Config\n",
    "# ROOT_DIR = Path(__file__).resolve().parents[2]  # Modified for notebook\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = Path('..').resolve()\n",
    "\n",
    "DIR_A = ROOT_DIR / \"data\" / \"tf_idf\"\n",
    "DIR_B = ROOT_DIR / \"data\" / \"tf_idf_manual\"\n",
    "\n",
    "\n",
    "MATRIX_FILE = \"tfidf_matrix.npz\"\n",
    "VECTORIZER_FILE = \"tfidf_vectorizer.joblib\"\n",
    "IDS_FILE = \"doc_ids.npy\"\n",
    "TITLES_FILE = \"doc_titles.npy\"\n",
    "\n",
    "TOLERANCE = 1e-7\n",
    "\n",
    "\n",
    "\n",
    "def compare_matrices(file_a, file_b):\n",
    "    \"\"\"compare two .npz sparse matrices\"\"\"\n",
    "    try:\n",
    "        A = sparse.load_npz(file_a)\n",
    "        B = sparse.load_npz(file_b)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"  [error] file not found: {e.filename}\")\n",
    "        return False\n",
    "    \n",
    "    # 1. check shape\n",
    "    if A.shape != B.shape:\n",
    "        print(f\"  [failed] shape mismatch: {A.shape} vs {B.shape}\")\n",
    "        return False\n",
    "    \n",
    "    # 2. check values\n",
    "    try:\n",
    "        C = A - B\n",
    "    except Exception as e:\n",
    "        print(f\"  [failed] matrix cannot be subtracted (format may be different?): {e}\")\n",
    "        return False\n",
    "        \n",
    "    if C.nnz == 0:\n",
    "        # no non-zero elements, means completely equal\n",
    "        return True\n",
    "        \n",
    "    # check if all difference values are almost 0\n",
    "    are_close = np.allclose(C.data, 0, atol=TOLERANCE)\n",
    "    if not are_close:\n",
    "        print(f\"  [failed] matrix has {C.nnz} values with difference (greater than {TOLERANCE}).\")\n",
    "        print(f\"  for example, the maximum difference value: {np.max(np.abs(C.data))}\")\n",
    "    return are_close\n",
    "\n",
    "def compare_vectorizers(file_a, file_b):\n",
    "    \"\"\"compare two .joblib vectorizers\"\"\"\n",
    "    try:\n",
    "        A = joblib.load(file_a)\n",
    "        B = joblib.load(file_b)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"  [error] file not found: {e.filename}\")\n",
    "        return False\n",
    "    \n",
    "    # 1. compare vocabulary (most important)\n",
    "    if A.vocabulary_ != B.vocabulary_:\n",
    "        print(f\"  [failed] vocabulary (vocabulary_) is inconsistent.\")\n",
    "        len_a = len(A.vocabulary_)\n",
    "        len_b = len(B.vocabulary_)\n",
    "        if len_a != len_b:\n",
    "            print(f\"  vocabulary size is different: {len_a} vs {len_b}\")\n",
    "        return False\n",
    "    \n",
    "    # 2. compare feature names (order must also be consistent)\n",
    "    try:\n",
    "        feats_a = A.get_feature_names_out()\n",
    "        feats_b = B.get_feature_names_out()\n",
    "        if not np.array_equal(feats_a, feats_b):\n",
    "            print(f\"  [failed] feature names (feature_names_out) are inconsistent.\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"  [warning] error comparing feature_names_out: {e}\")\n",
    "        \n",
    "    return True\n",
    "\n",
    "def compare_npy_arrays(file_a, file_b):\n",
    "    try:\n",
    "        A = np.load(file_a, allow_pickle=True) # allow_pickle 以防万一\n",
    "        B = np.load(file_b, allow_pickle=True)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"  [error] file not found: {e.filename}\")\n",
    "        return False\n",
    "        \n",
    "    if not np.array_equal(A, B):\n",
    "        print(f\"  [failed] array content is inconsistent.\")\n",
    "        if A.shape != B.shape:\n",
    "            print(f\"  shape mismatch: {A.shape} vs {B.shape}\")\n",
    "\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def main():\n",
    "    all_good = True\n",
    "    \n",
    "    print(f\"--- Comparing directories ---\")\n",
    "    print(f\"A: {DIR_A}\")\n",
    "    print(f\"B: {DIR_B}\")\n",
    "    \n",
    "    # 1. compare matrix\n",
    "    print(f\"\\n[1] compare {MATRIX_FILE}...\")\n",
    "    path_a = os.path.join(DIR_A, MATRIX_FILE)\n",
    "    path_b = os.path.join(DIR_B, MATRIX_FILE)\n",
    "    if compare_matrices(path_a, path_b):\n",
    "        print(f\"   {MATRIX_FILE} is consistent.\")\n",
    "    else:\n",
    "        print(f\"   {MATRIX_FILE} is inconsistent.\")\n",
    "        all_good = False\n",
    "        \n",
    "    # 2. compare vectorizer\n",
    "    print(f\"\\n[2] compare {VECTORIZER_FILE}...\")\n",
    "    path_a = os.path.join(DIR_A, VECTORIZER_FILE)\n",
    "    path_b = os.path.join(DIR_B, VECTORIZER_FILE)\n",
    "    if compare_vectorizers(path_a, path_b):\n",
    "        print(f\"  {VECTORIZER_FILE} is consistent.\")\n",
    "    else:\n",
    "        print(f\"  {VECTORIZER_FILE} is inconsistent.\")\n",
    "        all_good = False\n",
    "        \n",
    "    # 3. compare doc ids\n",
    "    print(f\"\\n[3] compare {IDS_FILE}...\")\n",
    "    path_a = os.path.join(DIR_A, IDS_FILE)\n",
    "    path_b = os.path.join(DIR_B, IDS_FILE)\n",
    "    if compare_npy_arrays(path_a, path_b):\n",
    "        print(f\"   {IDS_FILE} is consistent.\")\n",
    "    else:\n",
    "        print(f\"   {IDS_FILE} is inconsistent.\")\n",
    "        all_good = False\n",
    "        \n",
    "    # 4. compare doc titles\n",
    "    print(f\"\\n[4] compare {TITLES_FILE}...\")\n",
    "    path_a = os.path.join(DIR_A, TITLES_FILE)\n",
    "    path_b = os.path.join(DIR_B, TITLES_FILE)\n",
    "    if compare_npy_arrays(path_a, path_b):\n",
    "        print(f\"   {TITLES_FILE} is consistent.\")\n",
    "    else:\n",
    "        print(f\"   {TITLES_FILE} is inconsistent.\")\n",
    "        all_good = False\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"--- summary ---\")\n",
    "    if all_good:\n",
    "        print(\" all TF-IDF related files in both directories are consistent.\")\n",
    "    else:\n",
    "        print(\"differences found in both directories. Please check the ❌ marks above.\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa818c1",
   "metadata": {},
   "source": [
    "# lsa-kmeans\n",
    "- This module aims to reduce the dimensionality and remove noise from high-dimensional sparse TF-IDF matrices through Latent Semantic Analysis (LSA), extracting latent topic features from documents. Based on this, the K-Means algorithm is applied to perform unsupervised document clustering. The system comprises a complete pipeline: dimensionality selection detection, SVD decomposition, clustering execution, centroid backprojection interpretation, and density-based hierarchical refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13955d69",
   "metadata": {},
   "source": [
    "## LSA Clustering\n",
    "\n",
    "LSA maps the high-dimensional sparse TF-IDF matrix to a low-dimensional latent semantic space via singular value decomposition (SVD). TruncatedSVDprocesses the sparse matrix, compressing the high-dimensional feature space (100,000 dimensions) into a low-dimensional semantic space (n_components=1000). The reduced-dimension matrix X_reduced obtained via TruncatedSVD not only significantly saves storage space but also enhances the efficiency of similarity calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b330f5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] TF-IDF matrix loaded: shape=(732367, 100000), nnz=72361100\n",
      "[i] LSA reduction done: new shape=(732367, 1000)\n",
      "[i] Explained variance by top 1000 components: 25.83%\n",
      "[i] LSA reduced matrix saved to /work3/s242644/ds/PaperTrail/data/lsa_test/lsa_reduced.npz (dtype=float16)\n",
      "[i] LSA model (SVD object) saved to /work3/s242644/ds/PaperTrail/data/lsa_test/lsa_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# 1) Config\n",
    "# ROOT_DIR = Path(__file__).resolve().parents[2]  # Modified for notebook\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = Path('..').resolve()\n",
    "TFIDF_MATRIX_PATH = ROOT_DIR / \"data\" / \"tf_idf\" / \"tfidf_matrix.npz\"\n",
    "LSA_OUTPUT_PATH   = ROOT_DIR / \"data\" / \"lsa_test\" / \"lsa_reduced.npz\"\n",
    "LSA_MODEL_PATH = ROOT_DIR / \"data\" / \"lsa_test\" / \"lsa_model.joblib\"\n",
    "LSA_OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "LSA_MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Number of latent dimensions for LSA\n",
    "N_COMPONENTS = 1000  # (Adjustable like 100 or 200)\n",
    "\n",
    "def main():\n",
    "    # 1) Load the TF-IDF sparse matrix\n",
    "    X = sparse.load_npz(TFIDF_MATRIX_PATH)\n",
    "    print(f\"[i] TF-IDF matrix loaded: shape={X.shape}, nnz={X.nnz}\")\n",
    "\n",
    "    # 2) Perform LSA dimensionality reduction using TruncatedSVD\n",
    "    svd = TruncatedSVD(n_components=N_COMPONENTS, random_state=42)\n",
    "    X_reduced = svd.fit_transform(X)\n",
    "    print(f\"[i] LSA reduction done: new shape={X_reduced.shape}\")\n",
    "\n",
    "    # print explained variance ratio sum\n",
    "    if hasattr(svd, 'explained_variance_ratio_'):\n",
    "        variance_ratio_sum = svd.explained_variance_ratio_.sum()\n",
    "        print(f\"[i] Explained variance by top {N_COMPONENTS} components: {variance_ratio_sum:.2%}\")\n",
    "\n",
    "    # 3)Save the reduced matrix to a compressed NPZ file\n",
    "    # Convert to float16 to reduce file size (approx 50% of float32)\n",
    "    # Note: float16 is generally sufficient for LSA/embeddings and safer than int8 quantization\n",
    "    X_reduced = X_reduced.astype(np.float16)\n",
    "    np.savez_compressed(LSA_OUTPUT_PATH, X_reduced=X_reduced)\n",
    "    print(f\"[i] LSA reduced matrix saved to {LSA_OUTPUT_PATH} (dtype={X_reduced.dtype})\")\n",
    "    joblib.dump(svd, LSA_MODEL_PATH)\n",
    "    print(f\"[i] LSA model (SVD object) saved to {LSA_MODEL_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9147eb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from pathlib import Path\n",
    "#Config\n",
    "# ROOT_DIR = Path(__file__).resolve().parents[2]  # Modified for notebook\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = Path('..').resolve()\n",
    "LSA_INPUT_PATH = ROOT_DIR / \"data\" / \"lsa\" / \"lsa_reduced.npz\"\n",
    "CLUSTER_LABELS_PATH = ROOT_DIR / \"data\" / \"lsa\" / \"cluster_labels.npy\"\n",
    "\n",
    "K_FIXED = 40\n",
    "\n",
    "def main():\n",
    "    # 1) Load LSA reduced matrix\n",
    "    data = np.load(LSA_INPUT_PATH)\n",
    "    X = data['X_reduced']\n",
    "    print(f\"[i] Loaded LSA matrix: shape={X.shape}\")\n",
    "\n",
    "    print(f\"[i] Running MiniBatchKMeans with fixed K={K_FIXED}\")\n",
    "    kmeans = MiniBatchKMeans(n_clusters=K_FIXED, init='k-means++', n_init=10, random_state=42)\n",
    "    final_labels = kmeans.fit_predict(X)\n",
    "\n",
    "    inertia = kmeans.inertia_\n",
    "    silhouette = silhouette_score(X, final_labels)\n",
    "    print(f\"[i] Inertia={inertia:.2f}, Silhouette={silhouette:.4f}\")\n",
    "\n",
    "    np.save(CLUSTER_LABELS_PATH, final_labels)\n",
    "    print(f\"[i] Cluster labels saved to: {CLUSTER_LABELS_PATH}\")\n",
    "\n",
    "    # 4)Output Final clustering results:\n",
    "    unique, counts = np.unique(final_labels, return_counts=True)\n",
    "    print(\"Final clustering results:\")\n",
    "    for cid, count in zip(unique, counts):\n",
    "        print(f\" - Cluster {cid}: {count} papers\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4797fdfd",
   "metadata": {},
   "source": [
    "## kmeans\n",
    "Due to the large dataset size, we employed the MiniBatch K-Means algorithm for rapid iterative clustering. This algorithm maintains clustering performance comparable to standard K-Means while significantly reducing memory consumption and computational time. We partitioned the documents into 40 thematic clusters and projected the cluster centers back into the lexical space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7656d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Loaded LSA matrix: shape=(732367, 1000)\n",
      "[i] Running MiniBatchKMeans with fixed K=40\n",
      "[i] Cluster labels saved to: /work3/s242644/ds/PaperTrail/data/lsa/cluster_labels.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from pathlib import Path\n",
    "#Config\n",
    "# ROOT_DIR = Path(__file__).resolve().parents[2]  # Modified for notebook\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    ROOT_DIR = Path('..').resolve()\n",
    "LSA_INPUT_PATH = ROOT_DIR / \"data\" / \"lsa\" / \"lsa_reduced.npz\"\n",
    "CLUSTER_LABELS_PATH = ROOT_DIR / \"data\" / \"lsa\" / \"cluster_labels.npy\"\n",
    "\n",
    "K_FIXED = 40\n",
    "\n",
    "def main():\n",
    "    # 1) Load LSA reduced matrix\n",
    "    data = np.load(LSA_INPUT_PATH)\n",
    "    X = data['X_reduced']\n",
    "    print(f\"[i] Loaded LSA matrix: shape={X.shape}\")\n",
    "\n",
    "    print(f\"[i] Running MiniBatchKMeans with fixed K={K_FIXED}\")\n",
    "    kmeans = MiniBatchKMeans(n_clusters=K_FIXED, init='k-means++', n_init=10, random_state=42)\n",
    "    final_labels = kmeans.fit_predict(X)\n",
    "\n",
    "\n",
    "\n",
    "    np.save(CLUSTER_LABELS_PATH, final_labels)\n",
    "    print(f\"[i] Cluster labels saved to: {CLUSTER_LABELS_PATH}\")\n",
    "\n",
    "    # 4)Output Final clustering results:\n",
    "    unique, counts = np.unique(final_labels, return_counts=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a72c1a",
   "metadata": {},
   "source": [
    "# HDBSCAN Clustering\n",
    "\n",
    "**Due to the original dataset exceeding 700k data, running it directly in Jupyter will result in insufficient kernel memory. Therefore, we provide a TINY version with 10,000 records. For the full dataset version, please run src/lsa_and_clustering/sbert_hdbscan_cluster_lite.py.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Semantic Vectorization & Manifold Reduction\n",
    "This module employs a pre-trained Sentence Transformer model to encode text into semantic vectors. The selection of this model primarily balances computational efficiency with the ability to capture semantic context. The vectors are subsequently L2-normalized and projected into a low-dimensional space using the manifold learning algorithm UMAP (Uniform Manifold Approximation and Projection), constructing a compact manifold space for subsequent density-based clustering.\n",
    "\n",
    "2. Density-Based Clustering & Topic Mining\n",
    "The core clustering engine employs the HDBSCAN algorithm. HDBSCAN can adaptively identify clusters with uneven density in high-dimensional semantic spaces based on hierarchical structures, without requiring pre-set sensitive global distance thresholds. This enables greater robustness during automated parameter search. The system implements a sampled grid search mechanism (Sampled Parameter Search). Optimal parameter combinations are selected based on a composite evaluation metric Sopt, defined as a weighted linear combination of the silhouette coefficient and coverage. This automatically screens for parameter sets that balance intra-cluster compactness and sample coverage, while persisting search logs to CSV files for auditing.\n",
    "\n",
    "\n",
    "To generate semantically expressive cluster labels, we employ the Apriori association rule mining algorithm to identify frequent itemsets within each cluster as composite keyword tags (e.g., “neural + network”). By constructing detailed thematic labels from high-frequency co-occurring phrases, multi-word phrases, and term co-occurrence relationships, we enhance the interpretability of the recommendation system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83e9d87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SBERT + HDBSCAN (lite)\n",
      "============================================================\n",
      "\n",
      "[1] Loading texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSONL: 10000it [00:01, 7283.87it/s]\n",
      "/work3/s242644/seq_po_d1/seq_env/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] 10000 documents after filtering\n",
      "\n",
      "[2] Building/loading embeddings...\n",
      "[i] Loading cached embeddings: /work3/s242644/ds/PaperTrail/data/sbert_hdbscan_1w/sbert_embeddings.npy\n",
      "\n",
      "[3] UMAP (optional)...\n",
      "[i] UMAP pre-reduction...\n",
      "[i] Saved UMAP reducer model to: /work3/s242644/ds/PaperTrail/data/sbert_hdbscan_1w/umap_reducer.joblib\n",
      "[i] UMAP shape: (10000, 50)\n",
      "\n",
      "[4] Parameter search (sampled)...\n",
      "[i] Params used: {'min_cluster_size': 5, 'min_samples': 20, 'method': 'eom', 'eps': 0.0, 'metric': 'euclidean'}\n",
      "\n",
      "[5] Clustering on full data...\n",
      "[i] Saved HDBSCAN clusterer model to: /work3/s242644/ds/PaperTrail/data/sbert_hdbscan_1w/hdbscan_clusterer.joblib\n",
      "silhouette=0.5725  k=76  coverage=0.52  noise_rate=0.48\n",
      "\n",
      "[6] Saving artifacts...\n",
      "[i] saved: /work3/s242644/ds/PaperTrail/data/sbert_hdbscan_1w/cluster_top_terms.json\n",
      "[i] saved: /work3/s242644/ds/PaperTrail/data/sbert_hdbscan_1w/hdbscan_labels.npy\n",
      "[i] saved: /work3/s242644/ds/PaperTrail/data/sbert_hdbscan_1w/doc_ids.npy\n",
      "[i] saved: /work3/s242644/ds/PaperTrail/data/sbert_hdbscan_1w/doc_titles.npy\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import joblib\n",
    "import re\n",
    "import math\n",
    "from itertools import combinations\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import silhouette_score\n",
    "import csv\n",
    "import datetime\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"'force_all_finite' was renamed to 'ensure_all_finite'\"\n",
    ")\n",
    "\n",
    "class CsvLogger:\n",
    "    def __init__(self, filepath: Path, fieldnames: list):\n",
    "        self.filepath = filepath\n",
    "        self.fieldnames = fieldnames\n",
    "        with self.filepath.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "    def log(self, data: Dict[str, Any]):\n",
    "        # Ensure all keys in data are in fieldnames to avoid errors\n",
    "        filtered_data = {k: data.get(k) for k in self.fieldnames}\n",
    "        with self.filepath.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.fieldnames)\n",
    "            writer.writerow(filtered_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Paths\n",
    "# ROOT_DIR = Path(__file__).resolve().parents[2]  # Modified for notebook\n",
    "if 'ROOT_DIR' not in locals():\n",
    "    REPO_ROOT = Path('..').resolve()\n",
    "INPUT_JSONL = REPO_ROOT / \"data\" / \"preprocess\" / \"arxiv-cs-data-with-citations-final-dataset_preprocessed_head1w.json\"\n",
    "\n",
    "OUT_DIR = REPO_ROOT / \"data\" / \"sbert_hdbscan_1w\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EMBEDDINGS_PATH = OUT_DIR / \"sbert_embeddings.npy\"\n",
    "EMBEDDINGS_NORM_PATH = OUT_DIR / \"sbert_embeddings_norm.npy\"\n",
    "CLUSTER_LABELS_PATH = OUT_DIR / \"hdbscan_labels.npy\"\n",
    "DOC_IDS_PATH = OUT_DIR / \"doc_ids.npy\"\n",
    "DOC_TITLES_PATH = OUT_DIR / \"doc_titles.npy\"\n",
    "CLUSTER_TOP_TERMS_PATH = OUT_DIR / \"cluster_top_terms.json\"\n",
    "\n",
    "# Model and filters\n",
    "SBERT_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "MIN_TEXT_CHARS = 30\n",
    "CUSTOM_STOPWORDS_PATH = REPO_ROOT / \"src\" / \"custom_stopwords.txt\"\n",
    "APRIORI_MIN_SUPPORT_RATIO = 0.2\n",
    "APRIORI_MAX_SIZE = 3\n",
    "APRIORI_TOP_K = 5\n",
    "APRIORI_FALLBACK_TOP_K = 5\n",
    "\n",
    "# UMAP pre-reduction (fixed small config)\n",
    "UMAP_ENABLED = True\n",
    "UMAP_DIM = 50\n",
    "UMAP_METRIC = \"cosine\"  \n",
    "\n",
    "# Sampled parameter search (broader and smarter)\n",
    "PARAM_SEARCH = False\n",
    "PARAM_SAMPLE_SIZE = 10000\n",
    "# Baseline ranges (dynamic ranges are built inside search as well)\n",
    "MIN_CLUSTER_SIZE_RANGE = [5, 10, 20, 30, 50, 100]\n",
    "MIN_SAMPLES_RANGE = [1, 2, 5, 10, 15]\n",
    "METHODS = [\"eom\", \"leaf\"]\n",
    "EPS_RANGE = [0.0, 0.02, 0.05, 0.1, 0.2, 0.3]\n",
    "TARGET_SILHOUETTE = 1\n",
    "\n",
    "\n",
    "TOKEN_PATTERN = re.compile(r\"[a-z0-9]+\")\n",
    "_CUSTOM_STOP_WORDS: Optional[set[str]] = None\n",
    "\n",
    "\n",
    "def load_custom_stopwords() -> set[str]:\n",
    "    global _CUSTOM_STOP_WORDS\n",
    "    if _CUSTOM_STOP_WORDS is not None:\n",
    "        return _CUSTOM_STOP_WORDS\n",
    "    stops: set[str] = set()\n",
    "    try:\n",
    "        lines = CUSTOM_STOPWORDS_PATH.read_text(encoding=\"utf-8\").splitlines()\n",
    "    except FileNotFoundError:\n",
    "        _CUSTOM_STOP_WORDS = stops\n",
    "        return stops\n",
    "    for line in lines:\n",
    "        word = line.strip().lower()\n",
    "        if not word or word.startswith(\"#\"):\n",
    "            continue\n",
    "        stops.add(word)\n",
    "    _CUSTOM_STOP_WORDS = stops\n",
    "    return stops\n",
    "\n",
    "\n",
    "def tokenize_without_stopwords(text: str) -> Tuple[str, List[str]]:\n",
    "    text = (text or \"\").lower()\n",
    "    tokens = [m.group(0) for m in TOKEN_PATTERN.finditer(text)]\n",
    "    stops = load_custom_stopwords()\n",
    "    filtered = [tok for tok in tokens if tok not in stops and len(tok) > 1]\n",
    "    return \" \".join(filtered), filtered\n",
    "\n",
    "\n",
    "def apriori_frequent_itemsets(\n",
    "    transactions: List[List[str]],\n",
    "    min_support_ratio: float,\n",
    "    max_size: int,\n",
    "    top_k: int,\n",
    ") -> List[Tuple[Tuple[str, ...], int]]:\n",
    "    if not transactions:\n",
    "        return []\n",
    "    transactions_sets = [set(t) for t in transactions if t]\n",
    "    transactions_sets = [t for t in transactions_sets if t]\n",
    "    if not transactions_sets:\n",
    "        return []\n",
    "    n = len(transactions_sets)\n",
    "    min_support = max(2, int(math.ceil(min_support_ratio * n)))\n",
    "\n",
    "    # L1\n",
    "    item_counts = Counter()\n",
    "    for txn in transactions_sets:\n",
    "        for item in txn:\n",
    "            item_counts[(item,)] += 1\n",
    "    current_freq = {items: cnt for items, cnt in item_counts.items() if cnt >= min_support}\n",
    "    if not current_freq:\n",
    "        return []\n",
    "\n",
    "    freq_by_size: Dict[int, Dict[Tuple[str, ...], int]] = {1: current_freq}\n",
    "    all_freq: List[Tuple[Tuple[str, ...], int]] = list(current_freq.items())\n",
    "    k = 1\n",
    "\n",
    "    while k < max_size:\n",
    "        prev_freq = freq_by_size.get(k)\n",
    "        if not prev_freq:\n",
    "            break\n",
    "        prev_keys = list(prev_freq.keys())\n",
    "        candidates: set[Tuple[str, ...]] = set()\n",
    "        prev_key_sets = [set(key) for key in prev_keys]\n",
    "        for i in range(len(prev_keys)):\n",
    "            for j in range(i + 1, len(prev_keys)):\n",
    "                union_set = prev_key_sets[i] | prev_key_sets[j]\n",
    "                if len(union_set) != k + 1:\n",
    "                    continue\n",
    "                candidate = tuple(sorted(union_set))\n",
    "                # prune using Apriori property\n",
    "                if all(tuple(sorted(sub)) in prev_freq for sub in combinations(candidate, k)):\n",
    "                    candidates.add(candidate)\n",
    "        if not candidates:\n",
    "            break\n",
    "        counts = Counter()\n",
    "        for txn in transactions_sets:\n",
    "            for cand in candidates:\n",
    "                if set(cand).issubset(txn):\n",
    "                    counts[cand] += 1\n",
    "        next_freq = {cand: cnt for cand, cnt in counts.items() if cnt >= min_support}\n",
    "        if not next_freq:\n",
    "            break\n",
    "        k += 1\n",
    "        freq_by_size[k] = next_freq\n",
    "        all_freq.extend(next_freq.items())\n",
    "\n",
    "    # sort by (support desc, length desc, lexicographic)\n",
    "    all_freq.sort(key=lambda item: (-item[1], -len(item[0]), item[0]))\n",
    "    return all_freq[:top_k]\n",
    "\n",
    "\n",
    "def read_texts_ids_titles(jsonl_path: Path):\n",
    "    texts, ids, titles, token_lists = [], [], [], []\n",
    "    with jsonl_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, desc=\"Reading JSONL\"):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            raw_text = rec.get(\"processed_content\") or \"\"\n",
    "            _, tokens_text = tokenize_without_stopwords(raw_text)\n",
    "            raw_title = rec.get(\"title\") or \"\"\n",
    "            _, tokens_title = tokenize_without_stopwords(raw_title)\n",
    "            merged_tokens = tokens_title + tokens_text\n",
    "            merged_text = \" \".join(merged_tokens)\n",
    "            if len(raw_text) < MIN_TEXT_CHARS or not merged_tokens:\n",
    "                continue\n",
    "            texts.append(merged_text)\n",
    "            ids.append(rec.get(\"id\") or \"\")\n",
    "            titles.append(rec.get(\"title\") or \"\")\n",
    "            token_lists.append(merged_tokens)\n",
    "    return texts, np.array(ids), np.array(titles), token_lists\n",
    "\n",
    "\n",
    "def compute_cluster_top_terms(labels: np.ndarray, token_lists: List[List[str]], top_n: int = 10) -> Dict[int, List[str]]:\n",
    "    buckets: Dict[int, Counter] = defaultdict(Counter)\n",
    "    transactions_by_cluster: Dict[int, List[List[str]]] = defaultdict(list)\n",
    "    for lbl, tokens in zip(labels, token_lists):\n",
    "        if lbl is None:\n",
    "            continue\n",
    "        lbl_int = int(lbl)\n",
    "        if lbl_int < 0:\n",
    "            continue\n",
    "        if tokens:\n",
    "            buckets[lbl_int].update(tokens)\n",
    "            transactions_by_cluster[lbl_int].append(tokens)\n",
    "    topics: Dict[int, List[str]] = {}\n",
    "    for lbl, counter in buckets.items():\n",
    "        transactions = transactions_by_cluster.get(lbl, [])\n",
    "        itemsets = apriori_frequent_itemsets(\n",
    "            transactions,\n",
    "            min_support_ratio=APRIORI_MIN_SUPPORT_RATIO,\n",
    "            max_size=APRIORI_MAX_SIZE,\n",
    "            top_k=APRIORI_TOP_K,\n",
    "        )\n",
    "        if itemsets:\n",
    "            topics[lbl] = [\" + \".join(items) for items, _cnt in itemsets]\n",
    "        else:\n",
    "            # fallback to top individual terms if Apriori found nothing\n",
    "            topics[lbl] = [term for term, _ in counter.most_common(APRIORI_FALLBACK_TOP_K)]\n",
    "    return topics\n",
    "\n",
    "\n",
    "def build_or_load_embeddings(texts):\n",
    "    if EMBEDDINGS_PATH.exists():\n",
    "        print(f\"[i] Loading cached embeddings: {EMBEDDINGS_PATH}\")\n",
    "        X = np.load(EMBEDDINGS_PATH)\n",
    "    else:\n",
    "        print(f\"[i] Loading SBERT model: {SBERT_MODEL_NAME}\")\n",
    "        model = SentenceTransformer(SBERT_MODEL_NAME)\n",
    "        print(f\"[i] Encoding {len(texts)} documents...\")\n",
    "        X = model.encode(texts, convert_to_numpy=True, show_progress_bar=True, batch_size=32)\n",
    "        np.save(EMBEDDINGS_PATH, X)\n",
    "        print(f\"[i] Saved embeddings: {EMBEDDINGS_PATH}\")\n",
    "\n",
    "    X = normalize(X, norm=\"l2\", axis=1)\n",
    "    try:\n",
    "        np.save(EMBEDDINGS_NORM_PATH, X)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "\n",
    "def maybe_umap(X: np.ndarray) -> np.ndarray:\n",
    "    if not UMAP_ENABLED:\n",
    "        return X\n",
    "    try:\n",
    "        import umap\n",
    "    except Exception:\n",
    "        print(\"[warn] UMAP not installed, skipping pre-reduction\")\n",
    "        return X\n",
    "    print(\"[i] UMAP pre-reduction...\")\n",
    "    reducer = umap.UMAP(n_components=UMAP_DIM, metric=UMAP_METRIC, random_state=42 )\n",
    "    Z = reducer.fit_transform(X)\n",
    "    joblib.dump(reducer, OUT_DIR / \"umap_reducer.joblib\")\n",
    "    print(f\"[i] Saved UMAP reducer model to: {OUT_DIR / 'umap_reducer.joblib'}\")\n",
    "\n",
    "\n",
    "    Z = normalize(Z, norm=\"l2\", axis=1)\n",
    "    print(f\"[i] UMAP shape: {Z.shape}\")\n",
    "    return Z\n",
    "\n",
    "\n",
    "def run_hdbscan(X: np.ndarray, min_cluster_size: int, min_samples: int | None, method: str, eps: float):\n",
    "\n",
    "    kwargs = dict(min_cluster_size=min_cluster_size, metric=\"euclidean\", cluster_selection_method=method,\n",
    "                  prediction_data=True)\n",
    "    if min_samples is not None:\n",
    "        kwargs[\"min_samples\"] = min_samples\n",
    "    if eps and eps > 0:\n",
    "        kwargs[\"cluster_selection_epsilon\"] = eps\n",
    "    clusterer = hdbscan.HDBSCAN(**kwargs)\n",
    "    labels = clusterer.fit_predict(X)\n",
    "    return clusterer, labels\n",
    "\n",
    "\n",
    "def quick_metrics(X: np.ndarray, labels: np.ndarray):\n",
    "    n = len(labels)\n",
    "    noise = int((labels == -1).sum())\n",
    "    clustered = n - noise\n",
    "    k = len(np.unique(labels[labels != -1]))\n",
    "    sil = None\n",
    "    if clustered > 0 and k > 1:\n",
    "       \n",
    "        sil = float(silhouette_score(X[labels != -1], labels[labels != -1], metric=\"euclidean\"))\n",
    "    noise_rate = noise / n if n else 1.0\n",
    "    coverage = clustered / n if n else 0.0\n",
    "    return dict(n=n, noise=noise, clustered=clustered, k=k, silhouette=sil, noise_rate=noise_rate, coverage=coverage)\n",
    "\n",
    "\n",
    "def sampled_param_search(X: np.ndarray, csv_logger: Optional[CsvLogger] = None):\n",
    "    if not PARAM_SEARCH:\n",
    "        return None\n",
    "    Xs = X\n",
    "    if PARAM_SAMPLE_SIZE and X.shape[0] > PARAM_SAMPLE_SIZE:\n",
    "        rng = np.random.default_rng(42)\n",
    "        idx = rng.choice(X.shape[0], size=PARAM_SAMPLE_SIZE, replace=False)\n",
    "        Xs = X[idx]\n",
    "        print(f\"[i] Param search on sample: {Xs.shape[0]} / {X.shape[0]}\")\n",
    "\n",
    "    # Build dynamic ranges based on dataset size\n",
    "    n = Xs.shape[0]\n",
    "    dyn_mcs = sorted({\n",
    "        *MIN_CLUSTER_SIZE_RANGE,\n",
    "        max(5, n // 2000),\n",
    "        max(10, n // 1000),\n",
    "        max(20, n // 500),\n",
    "    })\n",
    "\n",
    "    def score(m):\n",
    "       \n",
    "        sil = m[\"silhouette\"] if m[\"silhouette\"] is not None else -1.0\n",
    "        coverage = m[\"coverage\"]\n",
    "        k = m[\"k\"]\n",
    "        k_pref = min(k / 50.0, 1.0)\n",
    "        return 0.7 * sil + 0.3 * coverage + 0.1 * k_pref\n",
    "\n",
    "    best = None\n",
    "    best_score = -1e9\n",
    "\n",
    "    \n",
    "    for mcs in dyn_mcs:\n",
    "        for ms in MIN_SAMPLES_RANGE:\n",
    "            for method in METHODS:\n",
    "                for eps in EPS_RANGE:\n",
    "                    try:\n",
    "                       \n",
    "                        metric = \"euclidean\"\n",
    "                        _, labels = run_hdbscan(Xs, mcs, ms, method, eps)\n",
    "                        metr = quick_metrics(Xs, labels) \n",
    "                        sc = score(metr)\n",
    "\n",
    "                        if csv_logger:\n",
    "                            log_data = {\n",
    "                                \"score\": sc,\n",
    "                                \"silhouette\": metr[\"silhouette\"],\n",
    "                                \"num_clusters\": metr[\"k\"],\n",
    "                                \"clustered_percentage\": metr[\"coverage\"],\n",
    "                                \"noise_rate\": metr[\"noise_rate\"],\n",
    "                                \"min_cluster_size\": mcs,\n",
    "                                \"min_samples\": ms,\n",
    "                                \"method\": method,\n",
    "                                \"eps\": eps,\n",
    "                                \"metric\": metric,\n",
    "                            }\n",
    "                            csv_logger.log(log_data)\n",
    "\n",
    "                        if sc > best_score:\n",
    "                            best_score = sc\n",
    "                            best = dict(min_cluster_size=mcs, min_samples=ms, method=method, eps=eps, metric=metric,\n",
    "                                        metrics=metr)\n",
    "                            sil_txt = \"NA\" if metr[\"silhouette\"] is None else f\"{metr['silhouette']:.4f}\"\n",
    "                            print(\n",
    "                                f\"  best so far: score={best_score:.4f} sil={sil_txt} k={metr['k']} coverage={metr['coverage']:.2f} noise_rate={metr['noise_rate']:.2f} params={{'min_cluster_size':{mcs},'min_samples':{ms},'method':'{method}','eps':{eps},'metric':'{metric}'}}\")\n",
    "\n",
    "                        if (metr[\"silhouette\"] is not None and metr[\"silhouette\"] >= 0.8 and\n",
    "                                metr[\"coverage\"] >= 0.9 and metr[\"k\"] <= 50):\n",
    "                            return best\n",
    "                    except Exception as e:\n",
    "                        print(f\"  skip error: {e}\")\n",
    "                        continue\n",
    "    return best\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SBERT + HDBSCAN (lite)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\n[1] Loading texts...\")\n",
    "    texts, ids, titles, token_lists = read_texts_ids_titles(INPUT_JSONL)\n",
    "    print(f\"[i] {len(texts)} documents after filtering\")\n",
    "\n",
    "    print(\"\\n[2] Building/loading embeddings...\")\n",
    "    X = build_or_load_embeddings(texts)\n",
    "\n",
    "    print(\"\\n[3] UMAP (optional)...\")\n",
    "    Z = maybe_umap(X)\n",
    "\n",
    "    print(\"\\n[4] Parameter search (sampled)...\")\n",
    "    csv_logger = None\n",
    "    if PARAM_SEARCH:\n",
    "        ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        LOG_CSV_PATH = OUT_DIR / f\"hdbscan_search_log_{ts}.csv\"\n",
    "        print(f\"[i] Logging parameter search to: {LOG_CSV_PATH}\")\n",
    "        log_fieldnames = [\n",
    "            \"score\", \"silhouette\", \"num_clusters\", \"clustered_percentage\", \"noise_rate\",\n",
    "            \"min_cluster_size\", \"min_samples\", \"method\", \"eps\", \"metric\"\n",
    "        ]\n",
    "        csv_logger = CsvLogger(LOG_CSV_PATH, log_fieldnames)\n",
    "\n",
    "    params = sampled_param_search(Z, csv_logger)\n",
    "    if params is None:\n",
    "       \n",
    "        params = dict(min_cluster_size=5, min_samples=20, method=\"eom\", eps=0.0, metric=\"euclidean\")\n",
    "\n",
    "   \n",
    "    try:\n",
    "        to_dump = dict(params={k: v for k, v in params.items() if k != \"metrics\"}, metrics=params.get(\"metrics\"))\n",
    "        with (OUT_DIR / \"hdbscan_params.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(to_dump, f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] Failed to save params json: {e}\")\n",
    "\n",
    "    \n",
    "    final_metric = params.get(\"metric\", \"euclidean\")\n",
    "    print(\n",
    "        f\"[i] Params used: {{'min_cluster_size': {params['min_cluster_size']}, 'min_samples': {params['min_samples']}, 'method': '{params['method']}', 'eps': {params['eps']}, 'metric': '{final_metric}'}}\")\n",
    "\n",
    "    print(\"\\n[5] Clustering on full data...\")\n",
    "    clusterer, labels = run_hdbscan(Z, params[\"min_cluster_size\"], params[\"min_samples\"], params[\"method\"],\n",
    "                                    params[\"eps\"])\n",
    "    joblib.dump(clusterer, OUT_DIR / \"hdbscan_clusterer.joblib\")\n",
    "    print(f\"[i] Saved HDBSCAN clusterer model to: {OUT_DIR / 'hdbscan_clusterer.joblib'}\")\n",
    "    metr = quick_metrics(Z, labels)  # Will use default 'euclidean'\n",
    "\n",
    "    sil_txt = \"NA\" if metr[\"silhouette\"] is None else f\"{metr['silhouette']:.4f}\"\n",
    "    print(f\"silhouette={sil_txt}  k={metr['k']}  coverage={metr['coverage']:.2f}  noise_rate={metr['noise_rate']:.2f}\")\n",
    "\n",
    "    # Save metrics summary\n",
    "    try:\n",
    "        with (OUT_DIR / \"hdbscan_metrics.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metr, f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] Failed to save metrics json: {e}\")\n",
    "\n",
    "    print(\"\\n[6] Saving artifacts...\")\n",
    "    np.save(CLUSTER_LABELS_PATH, labels)\n",
    "    np.save(DOC_IDS_PATH, ids)\n",
    "    np.save(DOC_TITLES_PATH, titles)\n",
    "\n",
    "    cluster_topics = compute_cluster_top_terms(labels, token_lists)\n",
    "    try:\n",
    "        with CLUSTER_TOP_TERMS_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(\n",
    "                {str(k): v for k, v in cluster_topics.items()},\n",
    "                f,\n",
    "                ensure_ascii=False,\n",
    "                indent=2,\n",
    "            )\n",
    "        print(f\"[i] saved: {CLUSTER_TOP_TERMS_PATH}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"[warn] failed to save cluster top terms: {exc}\")\n",
    "\n",
    "    print(f\"[i] saved: {CLUSTER_LABELS_PATH}\\n[i] saved: {DOC_IDS_PATH}\\n[i] saved: {DOC_TITLES_PATH}\")\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552c861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08447e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
